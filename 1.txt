








python3 -m sglang.launch_server --model-path /home/m/huggingface/Qwen3-0.6B --mem-fraction-static 0.5

GL_ENABLE_JIT_DEEPGEMM=0 python -m sglang.launch_server --model facebook/opt-125m --tp 1  --host 0.0.0.0 --port  5000 --mem-fraction-static 0.8 --context-length 64  --enable-metrics  --attention-backend triton --chunked-prefill-size 500






import openai
import os

from sglang.test.doc_patch import launch_server_cmd
from sglang.utils import wait_for_server, print_highlight, terminate_process

os.environ["TOKENIZERS_PARALLELISM"] = "false"


server_process, port = launch_server_cmd(
    "python -m sglang.launch_server --model-path facebook/opt-125m --host 0.0.0.0 --log-level warning"
)

wait_for_server(f"http://localhost:{port}")
client = openai.Client(base_url=f"http://127.0.0.1:{port}/v1", api_key="None")















import sglang as sgl
from sglang.srt.parser.reasoning_parser import ReasoningParser
from sglang.utils import print_highlight

llm = sgl.Engine(model_path="facebook/opt-125m")
tokenizer = AutoTokenizer.from_pretrained("facebook/opt-125m")
input = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True,
)
sampling_params = {
    "max_new_tokens": 1024,
    "skip_special_tokens": False,
    "temperature": 0.6,
    "top_p": 0.95,
}
result = llm.generate(prompt=input, sampling_params=sampling_params)

generated_text = result["text"]  # Assume there is only one prompt

print_highlight("==== Original Output ====")
print_highlight(generated_text)

parser = ReasoningParser("deepseek-r1")
reasoning_text, text = parser.parse_non_stream(generated_text)
print_highlight("==== Reasoning ====")
print_highlight(reasoning_text)
print_highlight("==== Text ====")
print_highlight(text







m@m-HP-Z440-Workstation:~/Desktop/sglang$ python3 -m venv venv && source venv/bin/activate
(venv) m@m-HP-Z440-Workstation:~/Desktop/sglang$ git clone -b v0.5.4.post1 https://github.com/sgl-project/sglang.git
cd sglang
Cloning into 'sglang'...
remote: Enumerating objects: 72566, done.
remote: Counting objects: 100% (794/794), done.
remote: Compressing objects: 100% (404/404), done.
remote: Total 72566 (delta 616), reused 457 (delta 390), pack-reused 71772 (from 4)
Receiving objects: 100% (72566/72566), 41.14 MiB | 3.41 MiB/s, done.
Resolving deltas: 100% (52885/52885), done.
Note: switching to '55d75e11bd32b6cabd03065afaa140fc3d969c69'.

You are in 'detached HEAD' state. You can look around, make experimental
changes and commit them, and you can discard any commits you make in this
state without impacting any branches by switching back to a branch.

If you want to create a new branch to retain commits you create, you may
do so (now or later) by using -c with the switch command. Example:

  git switch -c <new-branch-name>

Or undo this operation with:

  git switch -

Turn off this advice by setting config variable advice.detachedHead to false

(venv) m@m-HP-Z440-Workstation:~/Desktop/sglang/sglang$ pip install --upgrade pip
pip install -e "python"
Requirement already satisfied: pip in /home/m/Desktop/sglang/venv/lib/python3.12/site-packages (24.0)
Collecting pip
  Using cached pip-25.3-py3-none-any.whl.metadata (4.7 kB)
Using cached pip-25.3-py3-none-any.whl (1.8 MB)
Installing collected packages: pip
  Attempting uninstall: pip
    Found existing installation: pip 24.0
    Uninstalling pip-24.0:
      Successfully uninstalled pip-24.0
Successfully installed pip-25.3
Obtaining file:///home/m/Desktop/sglang/sglang/python
  Installing build dependencies ... done
  Checking if build backend supports build_editable ... done
  Getting requirements to build editable ... done
  Preparing editable metadata (pyproject.toml) ... done
Collecting IPython (from sglang==0.5.4.post1)
  Downloading ipython-9.6.0-py3-none-any.whl.metadata (4.4 kB)
Collecting aiohttp (from sglang==0.5.4.post1)
  Using cached aiohttp-3.13.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (8.1 kB)
Collecting anthropic>=0.20.0 (from sglang==0.5.4.post1)
  Downloading anthropic-0.72.0-py3-none-any.whl.metadata (28 kB)
Collecting blobfile==3.0.0 (from sglang==0.5.4.post1)
  Downloading blobfile-3.0.0-py3-none-any.whl.metadata (15 kB)
Collecting build (from sglang==0.5.4.post1)
  Using cached build-1.3.0-py3-none-any.whl.metadata (5.6 kB)
Collecting compressed-tensors (from sglang==0.5.4.post1)
  Downloading compressed_tensors-0.12.2-py3-none-any.whl.metadata (7.0 kB)
Collecting cuda-python (from sglang==0.5.4.post1)
  Downloading cuda_python-13.0.3-py3-none-any.whl.metadata (4.7 kB)
Collecting decord2 (from sglang==0.5.4.post1)
  Downloading decord2-2.0.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (686 bytes)
Collecting datasets (from sglang==0.5.4.post1)
  Downloading datasets-4.3.0-py3-none-any.whl.metadata (18 kB)
Collecting einops (from sglang==0.5.4.post1)
  Using cached einops-0.8.1-py3-none-any.whl.metadata (13 kB)
Collecting fastapi (from sglang==0.5.4.post1)
  Using cached fastapi-0.120.1-py3-none-any.whl.metadata (28 kB)
Collecting flashinfer_python==0.4.1 (from sglang==0.5.4.post1)
  Downloading flashinfer_python-0.4.1.tar.gz (4.6 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.6/4.6 MB 3.8 MB/s  0:00:01
  Installing build dependencies ... done
  Getting requirements to build wheel ... done
  Preparing metadata (pyproject.toml) ... done
Collecting gguf (from sglang==0.5.4.post1)
  Downloading gguf-0.17.1-py3-none-any.whl.metadata (4.3 kB)
Collecting hf_transfer (from sglang==0.5.4.post1)
  Downloading hf_transfer-0.1.9-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.7 kB)
Collecting huggingface_hub (from sglang==0.5.4.post1)
  Downloading huggingface_hub-1.0.1-py3-none-any.whl.metadata (13 kB)
Collecting interegular (from sglang==0.5.4.post1)
  Downloading interegular-0.3.3-py37-none-any.whl.metadata (3.0 kB)
Collecting llguidance<0.8.0,>=0.7.11 (from sglang==0.5.4.post1)
  Downloading llguidance-0.7.30-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)
Collecting modelscope (from sglang==0.5.4.post1)
  Downloading modelscope-1.31.0-py3-none-any.whl.metadata (40 kB)
Collecting msgspec (from sglang==0.5.4.post1)
  Downloading msgspec-0.19.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)
Collecting ninja (from sglang==0.5.4.post1)
  Using cached ninja-1.13.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (5.1 kB)
Collecting numpy (from sglang==0.5.4.post1)
  Using cached numpy-2.3.4-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (62 kB)
Collecting nvidia-cutlass-dsl==4.2.1 (from sglang==0.5.4.post1)
  Downloading nvidia_cutlass_dsl-4.2.1-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (2.5 kB)
Collecting openai-harmony==0.0.4 (from sglang==0.5.4.post1)
  Using cached openai_harmony-0.0.4-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.0 kB)
Collecting openai==1.99.1 (from sglang==0.5.4.post1)
  Downloading openai-1.99.1-py3-none-any.whl.metadata (29 kB)
Collecting orjson (from sglang==0.5.4.post1)
  Downloading orjson-3.11.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (41 kB)
Collecting outlines==0.1.11 (from sglang==0.5.4.post1)
  Downloading outlines-0.1.11-py3-none-any.whl.metadata (17 kB)
Collecting packaging (from sglang==0.5.4.post1)
  Using cached packaging-25.0-py3-none-any.whl.metadata (3.3 kB)
Collecting partial_json_parser (from sglang==0.5.4.post1)
  Downloading partial_json_parser-0.2.1.1.post6-py3-none-any.whl.metadata (6.1 kB)
Collecting pillow (from sglang==0.5.4.post1)
  Downloading pillow-12.0.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (8.8 kB)
Collecting prometheus-client>=0.20.0 (from sglang==0.5.4.post1)
  Using cached prometheus_client-0.23.1-py3-none-any.whl.metadata (1.9 kB)
Collecting psutil (from sglang==0.5.4.post1)
  Using cached psutil-7.1.2-cp36-abi3-manylinux2010_x86_64.manylinux_2_12_x86_64.manylinux_2_28_x86_64.whl.metadata (23 kB)
Collecting py-spy (from sglang==0.5.4.post1)
  Downloading py_spy-0.4.1-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl.metadata (510 bytes)
Collecting pybase64 (from sglang==0.5.4.post1)
  Downloading pybase64-1.4.2-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl.metadata (8.7 kB)
Collecting pydantic (from sglang==0.5.4.post1)
  Using cached pydantic-2.12.3-py3-none-any.whl.metadata (87 kB)
Collecting nvidia-ml-py (from sglang==0.5.4.post1)
  Downloading nvidia_ml_py-13.580.82-py3-none-any.whl.metadata (9.6 kB)
Collecting python-multipart (from sglang==0.5.4.post1)
  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)
Collecting pyzmq>=25.1.2 (from sglang==0.5.4.post1)
  Using cached pyzmq-27.1.0-cp312-abi3-manylinux_2_26_x86_64.manylinux_2_28_x86_64.whl.metadata (6.0 kB)
Collecting requests (from sglang==0.5.4.post1)
  Using cached requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)
Collecting scipy (from sglang==0.5.4.post1)
  Downloading scipy-1.16.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (62 kB)
Collecting sentencepiece (from sglang==0.5.4.post1)
  Using cached sentencepiece-0.2.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (10 kB)
Collecting setproctitle (from sglang==0.5.4.post1)
  Downloading setproctitle-1.3.7-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (10 kB)
Collecting sgl-kernel==0.3.16.post4 (from sglang==0.5.4.post1)
  Downloading sgl_kernel-0.3.16.post4-cp310-abi3-manylinux2014_x86_64.whl.metadata (17 kB)
Collecting soundfile==0.13.1 (from sglang==0.5.4.post1)
  Using cached soundfile-0.13.1-py2.py3-none-manylinux_2_28_x86_64.whl.metadata (16 kB)
Collecting tiktoken (from sglang==0.5.4.post1)
  Using cached tiktoken-0.12.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (6.7 kB)
Collecting timm==1.0.16 (from sglang==0.5.4.post1)
  Downloading timm-1.0.16-py3-none-any.whl.metadata (57 kB)
Collecting torch==2.8.0 (from sglang==0.5.4.post1)
  Downloading torch-2.8.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (30 kB)
Collecting torch_memory_saver==0.0.9 (from sglang==0.5.4.post1)
  Downloading torch_memory_saver-0.0.9-cp39-abi3-manylinux2014_x86_64.whl.metadata (108 bytes)
Collecting torchao==0.9.0 (from sglang==0.5.4.post1)
  Downloading torchao-0.9.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl.metadata (14 kB)
Collecting torchaudio==2.8.0 (from sglang==0.5.4.post1)
  Downloading torchaudio-2.8.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (7.2 kB)
Collecting torchvision (from sglang==0.5.4.post1)
  Using cached torchvision-0.24.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (5.9 kB)
Collecting tqdm (from sglang==0.5.4.post1)
  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)
Collecting transformers==4.57.1 (from sglang==0.5.4.post1)
  Using cached transformers-4.57.1-py3-none-any.whl.metadata (43 kB)
Collecting uvicorn (from sglang==0.5.4.post1)
  Using cached uvicorn-0.38.0-py3-none-any.whl.metadata (6.8 kB)
Collecting uvloop (from sglang==0.5.4.post1)
  Using cached uvloop-0.22.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (4.9 kB)
Collecting xgrammar==0.1.25 (from sglang==0.5.4.post1)
  Downloading xgrammar-0.1.25-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.5 kB)
Collecting grpcio==1.75.1 (from sglang==0.5.4.post1)
  Downloading grpcio-1.75.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.7 kB)
Collecting grpcio-tools==1.75.1 (from sglang==0.5.4.post1)
  Downloading grpcio_tools-1.75.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (5.3 kB)
Collecting grpcio-reflection==1.75.1 (from sglang==0.5.4.post1)
  Downloading grpcio_reflection-1.75.1-py3-none-any.whl.metadata (1.1 kB)
Collecting grpcio-health-checking==1.75.1 (from sglang==0.5.4.post1)
  Downloading grpcio_health_checking-1.75.1-py3-none-any.whl.metadata (1.1 kB)
Collecting pycryptodomex>=3.8 (from blobfile==3.0.0->sglang==0.5.4.post1)
  Using cached pycryptodomex-3.23.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)
Collecting urllib3<3,>=1.25.3 (from blobfile==3.0.0->sglang==0.5.4.post1)
  Using cached urllib3-2.5.0-py3-none-any.whl.metadata (6.5 kB)
Collecting lxml>=4.9 (from blobfile==3.0.0->sglang==0.5.4.post1)
  Using cached lxml-6.0.2-cp312-cp312-manylinux_2_26_x86_64.manylinux_2_28_x86_64.whl.metadata (3.6 kB)
Collecting filelock>=3.0 (from blobfile==3.0.0->sglang==0.5.4.post1)
  Using cached filelock-3.20.0-py3-none-any.whl.metadata (2.1 kB)
Collecting apache-tvm-ffi==0.1.0b15 (from flashinfer_python==0.4.1->sglang==0.5.4.post1)
  Using cached apache_tvm_ffi-0.1.0b15-cp312-abi3-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (2.0 kB)
Collecting click (from flashinfer_python==0.4.1->sglang==0.5.4.post1)
  Using cached click-8.3.0-py3-none-any.whl.metadata (2.6 kB)
Collecting nvidia-cudnn-frontend>=1.13.0 (from flashinfer_python==0.4.1->sglang==0.5.4.post1)
  Downloading nvidia_cudnn_frontend-1.15.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (6.7 kB)
Collecting tabulate (from flashinfer_python==0.4.1->sglang==0.5.4.post1)
  Downloading tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)
Collecting typing-extensions>=4.5 (from apache-tvm-ffi==0.1.0b15->flashinfer_python==0.4.1->sglang==0.5.4.post1)
  Using cached typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)
Collecting protobuf<7.0.0,>=6.31.1 (from grpcio-health-checking==1.75.1->sglang==0.5.4.post1)
  Using cached protobuf-6.33.0-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)
Collecting setuptools (from grpcio-tools==1.75.1->sglang==0.5.4.post1)
  Using cached setuptools-80.9.0-py3-none-any.whl.metadata (6.6 kB)
Collecting anyio<5,>=3.5.0 (from openai==1.99.1->sglang==0.5.4.post1)
  Using cached anyio-4.11.0-py3-none-any.whl.metadata (4.1 kB)
Collecting distro<2,>=1.7.0 (from openai==1.99.1->sglang==0.5.4.post1)
  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)
Collecting httpx<1,>=0.23.0 (from openai==1.99.1->sglang==0.5.4.post1)
  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)
Collecting jiter<1,>=0.4.0 (from openai==1.99.1->sglang==0.5.4.post1)
  Using cached jiter-0.11.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)
Collecting sniffio (from openai==1.99.1->sglang==0.5.4.post1)
  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)
Collecting jinja2 (from outlines==0.1.11->sglang==0.5.4.post1)
  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)
Collecting lark (from outlines==0.1.11->sglang==0.5.4.post1)
  Using cached lark-1.3.1-py3-none-any.whl.metadata (1.8 kB)
Collecting nest_asyncio (from outlines==0.1.11->sglang==0.5.4.post1)
  Downloading nest_asyncio-1.6.0-py3-none-any.whl.metadata (2.8 kB)
Collecting cloudpickle (from outlines==0.1.11->sglang==0.5.4.post1)
  Downloading cloudpickle-3.1.1-py3-none-any.whl.metadata (7.1 kB)
Collecting diskcache (from outlines==0.1.11->sglang==0.5.4.post1)
  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)
Collecting referencing (from outlines==0.1.11->sglang==0.5.4.post1)
  Using cached referencing-0.37.0-py3-none-any.whl.metadata (2.8 kB)
Collecting jsonschema (from outlines==0.1.11->sglang==0.5.4.post1)
  Using cached jsonschema-4.25.1-py3-none-any.whl.metadata (7.6 kB)
Collecting pycountry (from outlines==0.1.11->sglang==0.5.4.post1)
  Downloading pycountry-24.6.1-py3-none-any.whl.metadata (12 kB)
Collecting airportsdata (from outlines==0.1.11->sglang==0.5.4.post1)
  Downloading airportsdata-20250909-py3-none-any.whl.metadata (9.2 kB)
Collecting outlines_core==0.1.26 (from outlines==0.1.11->sglang==0.5.4.post1)
  Downloading outlines_core-0.1.26-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)
Collecting cffi>=1.0 (from soundfile==0.13.1->sglang==0.5.4.post1)
  Using cached cffi-2.0.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.6 kB)
Collecting pyyaml (from timm==1.0.16->sglang==0.5.4.post1)
  Using cached pyyaml-6.0.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (2.4 kB)
Collecting safetensors (from timm==1.0.16->sglang==0.5.4.post1)
  Using cached safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)
Collecting sympy>=1.13.3 (from torch==2.8.0->sglang==0.5.4.post1)
  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)
Collecting networkx (from torch==2.8.0->sglang==0.5.4.post1)
  Using cached networkx-3.5-py3-none-any.whl.metadata (6.3 kB)
Collecting fsspec (from torch==2.8.0->sglang==0.5.4.post1)
  Using cached fsspec-2025.9.0-py3-none-any.whl.metadata (10 kB)
Collecting nvidia-cuda-nvrtc-cu12==12.8.93 (from torch==2.8.0->sglang==0.5.4.post1)
  Using cached nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)
Collecting nvidia-cuda-runtime-cu12==12.8.90 (from torch==2.8.0->sglang==0.5.4.post1)
  Using cached nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)
Collecting nvidia-cuda-cupti-cu12==12.8.90 (from torch==2.8.0->sglang==0.5.4.post1)
  Using cached nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)
Collecting nvidia-cudnn-cu12==9.10.2.21 (from torch==2.8.0->sglang==0.5.4.post1)
  Using cached nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)
Collecting nvidia-cublas-cu12==12.8.4.1 (from torch==2.8.0->sglang==0.5.4.post1)
  Using cached nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)
Collecting nvidia-cufft-cu12==11.3.3.83 (from torch==2.8.0->sglang==0.5.4.post1)
  Using cached nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)
Collecting nvidia-curand-cu12==10.3.9.90 (from torch==2.8.0->sglang==0.5.4.post1)
  Using cached nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)
Collecting nvidia-cusolver-cu12==11.7.3.90 (from torch==2.8.0->sglang==0.5.4.post1)
  Using cached nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)
Collecting nvidia-cusparse-cu12==12.5.8.93 (from torch==2.8.0->sglang==0.5.4.post1)
  Using cached nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)
Collecting nvidia-cusparselt-cu12==0.7.1 (from torch==2.8.0->sglang==0.5.4.post1)
  Using cached nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl.metadata (7.0 kB)
Collecting nvidia-nccl-cu12==2.27.3 (from torch==2.8.0->sglang==0.5.4.post1)
  Downloading nvidia_nccl_cu12-2.27.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)
Collecting nvidia-nvtx-cu12==12.8.90 (from torch==2.8.0->sglang==0.5.4.post1)
  Using cached nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)
Collecting nvidia-nvjitlink-cu12==12.8.93 (from torch==2.8.0->sglang==0.5.4.post1)
  Using cached nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)
Collecting nvidia-cufile-cu12==1.13.1.3 (from torch==2.8.0->sglang==0.5.4.post1)
  Using cached nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)
Collecting triton==3.4.0 (from torch==2.8.0->sglang==0.5.4.post1)
  Downloading triton-3.4.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.7 kB)
Collecting huggingface_hub (from sglang==0.5.4.post1)
  Using cached huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)
Collecting regex!=2019.12.17 (from transformers==4.57.1->sglang==0.5.4.post1)
  Using cached regex-2025.10.23-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)
Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers==4.57.1->sglang==0.5.4.post1)
  Using cached tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)
Collecting idna>=2.8 (from anyio<5,>=3.5.0->openai==1.99.1->sglang==0.5.4.post1)
  Using cached idna-3.11-py3-none-any.whl.metadata (8.4 kB)
Collecting certifi (from httpx<1,>=0.23.0->openai==1.99.1->sglang==0.5.4.post1)
  Using cached certifi-2025.10.5-py3-none-any.whl.metadata (2.5 kB)
Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai==1.99.1->sglang==0.5.4.post1)
  Using cached httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)
Collecting h11>=0.16 (from httpcore==1.*->httpx<1,>=0.23.0->openai==1.99.1->sglang==0.5.4.post1)
  Using cached h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)
Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface_hub->sglang==0.5.4.post1)
  Using cached hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)
Collecting annotated-types>=0.6.0 (from pydantic->sglang==0.5.4.post1)
  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)
Collecting pydantic-core==2.41.4 (from pydantic->sglang==0.5.4.post1)
  Using cached pydantic_core-2.41.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.3 kB)
Collecting typing-inspection>=0.4.2 (from pydantic->sglang==0.5.4.post1)
  Using cached typing_inspection-0.4.2-py3-none-any.whl.metadata (2.6 kB)
Collecting docstring-parser<1,>=0.15 (from anthropic>=0.20.0->sglang==0.5.4.post1)
  Downloading docstring_parser-0.17.0-py3-none-any.whl.metadata (3.5 kB)
Collecting pycparser (from cffi>=1.0->soundfile==0.13.1->sglang==0.5.4.post1)
  Using cached pycparser-2.23-py3-none-any.whl.metadata (993 bytes)
Collecting cuda-bindings~=13.0.3 (from cuda-python->sglang==0.5.4.post1)
  Downloading cuda_bindings-13.0.3-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (2.3 kB)
Collecting cuda-pathfinder~=1.1 (from cuda-python->sglang==0.5.4.post1)
  Using cached cuda_pathfinder-1.3.1-py3-none-any.whl.metadata (1.9 kB)
Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch==2.8.0->sglang==0.5.4.post1)
  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)
Collecting aiohappyeyeballs>=2.5.0 (from aiohttp->sglang==0.5.4.post1)
  Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)
Collecting aiosignal>=1.4.0 (from aiohttp->sglang==0.5.4.post1)
  Using cached aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)
Collecting attrs>=17.3.0 (from aiohttp->sglang==0.5.4.post1)
  Using cached attrs-25.4.0-py3-none-any.whl.metadata (10 kB)
Collecting frozenlist>=1.1.1 (from aiohttp->sglang==0.5.4.post1)
  Using cached frozenlist-1.8.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (20 kB)
Collecting multidict<7.0,>=4.5 (from aiohttp->sglang==0.5.4.post1)
  Using cached multidict-6.7.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.3 kB)
Collecting propcache>=0.2.0 (from aiohttp->sglang==0.5.4.post1)
  Using cached propcache-0.4.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (13 kB)
Collecting yarl<2.0,>=1.17.0 (from aiohttp->sglang==0.5.4.post1)
  Using cached yarl-1.22.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (75 kB)
Collecting pyproject_hooks (from build->sglang==0.5.4.post1)
  Using cached pyproject_hooks-1.2.0-py3-none-any.whl.metadata (1.3 kB)
Collecting loguru (from compressed-tensors->sglang==0.5.4.post1)
  Downloading loguru-0.7.3-py3-none-any.whl.metadata (22 kB)
Collecting pyarrow>=21.0.0 (from datasets->sglang==0.5.4.post1)
  Using cached pyarrow-22.0.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.2 kB)
Collecting dill<0.4.1,>=0.3.0 (from datasets->sglang==0.5.4.post1)
  Downloading dill-0.4.0-py3-none-any.whl.metadata (10 kB)
Collecting pandas (from datasets->sglang==0.5.4.post1)
  Using cached pandas-2.3.3-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (91 kB)
Collecting xxhash (from datasets->sglang==0.5.4.post1)
  Using cached xxhash-3.6.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (13 kB)
Collecting multiprocess<0.70.17 (from datasets->sglang==0.5.4.post1)
  Using cached multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)
Collecting charset_normalizer<4,>=2 (from requests->sglang==0.5.4.post1)
  Using cached charset_normalizer-3.4.4-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (37 kB)
Collecting starlette<0.50.0,>=0.40.0 (from fastapi->sglang==0.5.4.post1)
  Downloading starlette-0.49.1-py3-none-any.whl.metadata (6.4 kB)
Collecting annotated-doc>=0.0.2 (from fastapi->sglang==0.5.4.post1)
  Downloading annotated_doc-0.0.3-py3-none-any.whl.metadata (6.6 kB)
Collecting decorator (from IPython->sglang==0.5.4.post1)
  Downloading decorator-5.2.1-py3-none-any.whl.metadata (3.9 kB)
Collecting ipython-pygments-lexers (from IPython->sglang==0.5.4.post1)
  Downloading ipython_pygments_lexers-1.1.1-py3-none-any.whl.metadata (1.1 kB)
Collecting jedi>=0.16 (from IPython->sglang==0.5.4.post1)
  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)
Collecting matplotlib-inline (from IPython->sglang==0.5.4.post1)
  Downloading matplotlib_inline-0.2.1-py3-none-any.whl.metadata (2.3 kB)
Collecting pexpect>4.3 (from IPython->sglang==0.5.4.post1)
  Downloading pexpect-4.9.0-py2.py3-none-any.whl.metadata (2.5 kB)
Collecting prompt_toolkit<3.1.0,>=3.0.41 (from IPython->sglang==0.5.4.post1)
  Downloading prompt_toolkit-3.0.52-py3-none-any.whl.metadata (6.4 kB)
Collecting pygments>=2.4.0 (from IPython->sglang==0.5.4.post1)
  Using cached pygments-2.19.2-py3-none-any.whl.metadata (2.5 kB)
Collecting stack_data (from IPython->sglang==0.5.4.post1)
  Downloading stack_data-0.6.3-py3-none-any.whl.metadata (18 kB)
Collecting traitlets>=5.13.0 (from IPython->sglang==0.5.4.post1)
  Downloading traitlets-5.14.3-py3-none-any.whl.metadata (10 kB)
Collecting wcwidth (from prompt_toolkit<3.1.0,>=3.0.41->IPython->sglang==0.5.4.post1)
  Downloading wcwidth-0.2.14-py2.py3-none-any.whl.metadata (15 kB)
Collecting parso<0.9.0,>=0.8.4 (from jedi>=0.16->IPython->sglang==0.5.4.post1)
  Downloading parso-0.8.5-py2.py3-none-any.whl.metadata (8.3 kB)
Collecting ptyprocess>=0.5 (from pexpect>4.3->IPython->sglang==0.5.4.post1)
  Downloading ptyprocess-0.7.0-py2.py3-none-any.whl.metadata (1.3 kB)
Collecting MarkupSafe>=2.0 (from jinja2->outlines==0.1.11->sglang==0.5.4.post1)
  Using cached markupsafe-3.0.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (2.7 kB)
Collecting jsonschema-specifications>=2023.03.6 (from jsonschema->outlines==0.1.11->sglang==0.5.4.post1)
  Using cached jsonschema_specifications-2025.9.1-py3-none-any.whl.metadata (2.9 kB)
Collecting rpds-py>=0.7.1 (from jsonschema->outlines==0.1.11->sglang==0.5.4.post1)
  Using cached rpds_py-0.28.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)
Collecting python-dateutil>=2.8.2 (from pandas->datasets->sglang==0.5.4.post1)
  Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)
Collecting pytz>=2020.1 (from pandas->datasets->sglang==0.5.4.post1)
  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)
Collecting tzdata>=2022.7 (from pandas->datasets->sglang==0.5.4.post1)
  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)
Collecting six>=1.5 (from python-dateutil>=2.8.2->pandas->datasets->sglang==0.5.4.post1)
  Using cached six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)
Collecting executing>=1.2.0 (from stack_data->IPython->sglang==0.5.4.post1)
  Downloading executing-2.2.1-py2.py3-none-any.whl.metadata (8.9 kB)
Collecting asttokens>=2.1.0 (from stack_data->IPython->sglang==0.5.4.post1)
  Downloading asttokens-3.0.0-py3-none-any.whl.metadata (4.7 kB)
Collecting pure-eval (from stack_data->IPython->sglang==0.5.4.post1)
  Downloading pure_eval-0.2.3-py3-none-any.whl.metadata (6.3 kB)
INFO: pip is looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.
Collecting torchvision (from sglang==0.5.4.post1)
  Using cached torchvision-0.23.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (6.1 kB)
Downloading blobfile-3.0.0-py3-none-any.whl (75 kB)
Using cached apache_tvm_ffi-0.1.0b15-cp312-abi3-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (1.6 MB)
Downloading grpcio-1.75.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (6.4 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.4/6.4 MB 2.3 MB/s  0:00:02
Downloading grpcio_health_checking-1.75.1-py3-none-any.whl (18 kB)
Downloading grpcio_reflection-1.75.1-py3-none-any.whl (22 kB)
Downloading grpcio_tools-1.75.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (2.7 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.7/2.7 MB 3.3 MB/s  0:00:00
Downloading nvidia_cutlass_dsl-4.2.1-cp312-cp312-manylinux_2_28_x86_64.whl (62.2 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 62.2/62.2 MB 3.2 MB/s  0:00:19
Downloading openai-1.99.1-py3-none-any.whl (767 kB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 767.8/767.8 kB 2.8 MB/s  0:00:00
Using cached openai_harmony-0.0.4-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)
Downloading outlines-0.1.11-py3-none-any.whl (87 kB)
Downloading outlines_core-0.1.26-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (343 kB)
Downloading sgl_kernel-0.3.16.post4-cp310-abi3-manylinux2014_x86_64.whl (510.4 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 510.4/510.4 MB 3.1 MB/s  0:02:28
Using cached soundfile-0.13.1-py2.py3-none-manylinux_2_28_x86_64.whl (1.3 MB)
Downloading timm-1.0.16-py3-none-any.whl (2.5 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.5/2.5 MB 3.6 MB/s  0:00:00
Downloading torch-2.8.0-cp312-cp312-manylinux_2_28_x86_64.whl (887.9 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 887.9/887.9 MB 3.2 MB/s  0:04:17
Using cached nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl (594.3 MB)
Using cached nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (10.2 MB)
Using cached nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (88.0 MB)
Using cached nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (954 kB)
Using cached nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl (706.8 MB)
Using cached nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (193.1 MB)
Using cached nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.2 MB)
Using cached nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl (63.6 MB)
Using cached nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl (267.5 MB)
Using cached nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (288.2 MB)
Using cached nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl (287.2 MB)
Downloading nvidia_nccl_cu12-2.27.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (322.4 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 322.4/322.4 MB 1.1 MB/s  0:01:52
Using cached nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.3 MB)
Using cached nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)
Downloading torch_memory_saver-0.0.9-cp39-abi3-manylinux2014_x86_64.whl (488 kB)
Downloading torchao-0.9.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl (5.7 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.7/5.7 MB 3.8 MB/s  0:00:01
Downloading torchaudio-2.8.0-cp312-cp312-manylinux_2_28_x86_64.whl (4.0 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.0/4.0 MB 3.5 MB/s  0:00:01
Using cached transformers-4.57.1-py3-none-any.whl (12.0 MB)
Downloading triton-3.4.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (155.6 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 155.6/155.6 MB 2.6 MB/s  0:00:52
Downloading xgrammar-0.1.25-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.7 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 8.7/8.7 MB 3.3 MB/s  0:00:02
Using cached anyio-4.11.0-py3-none-any.whl (109 kB)
Using cached distro-1.9.0-py3-none-any.whl (20 kB)
Using cached httpx-0.28.1-py3-none-any.whl (73 kB)
Using cached httpcore-1.0.9-py3-none-any.whl (78 kB)
Using cached huggingface_hub-0.36.0-py3-none-any.whl (566 kB)
Using cached hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)
Using cached jiter-0.11.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (358 kB)
Downloading llguidance-0.7.30-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.0 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 15.0/15.0 MB 3.0 MB/s  0:00:05
Using cached protobuf-6.33.0-cp39-abi3-manylinux2014_x86_64.whl (323 kB)
Using cached pydantic-2.12.3-py3-none-any.whl (462 kB)
Using cached pydantic_core-2.41.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)
Using cached tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)
Using cached typing_extensions-4.15.0-py3-none-any.whl (44 kB)
Using cached urllib3-2.5.0-py3-none-any.whl (129 kB)
Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)
Downloading anthropic-0.72.0-py3-none-any.whl (357 kB)
Downloading docstring_parser-0.17.0-py3-none-any.whl (36 kB)
Using cached cffi-2.0.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (219 kB)
Downloading cuda_python-13.0.3-py3-none-any.whl (7.6 kB)
Downloading cuda_bindings-13.0.3-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (12.1 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.1/12.1 MB 3.1 MB/s  0:00:03
Using cached cuda_pathfinder-1.3.1-py3-none-any.whl (27 kB)
Using cached filelock-3.20.0-py3-none-any.whl (16 kB)
Using cached fsspec-2025.9.0-py3-none-any.whl (199 kB)
Using cached h11-0.16.0-py3-none-any.whl (37 kB)
Using cached idna-3.11-py3-none-any.whl (71 kB)
Using cached lxml-6.0.2-cp312-cp312-manylinux_2_26_x86_64.manylinux_2_28_x86_64.whl (5.3 MB)
Using cached numpy-2.3.4-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.6 MB)
Downloading nvidia_cudnn_frontend-1.15.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (1.9 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.9/1.9 MB 3.0 MB/s  0:00:00
Using cached packaging-25.0-py3-none-any.whl (66 kB)
Using cached prometheus_client-0.23.1-py3-none-any.whl (61 kB)
Using cached pycryptodomex-3.23.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)
Using cached pyyaml-6.0.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (807 kB)
Using cached pyzmq-27.1.0-cp312-abi3-manylinux_2_26_x86_64.manylinux_2_28_x86_64.whl (840 kB)
Using cached regex-2025.10.23-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (803 kB)
Using cached safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (485 kB)
Using cached setuptools-80.9.0-py3-none-any.whl (1.2 MB)
Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)
Using cached sympy-1.14.0-py3-none-any.whl (6.3 MB)
Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)
Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)
Using cached typing_inspection-0.4.2-py3-none-any.whl (14 kB)
Using cached aiohttp-3.13.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (1.8 MB)
Using cached multidict-6.7.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (256 kB)
Using cached yarl-1.22.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (377 kB)
Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)
Using cached aiosignal-1.4.0-py3-none-any.whl (7.5 kB)
Using cached attrs-25.4.0-py3-none-any.whl (67 kB)
Using cached frozenlist-1.8.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (242 kB)
Using cached propcache-0.4.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (221 kB)
Downloading airportsdata-20250909-py3-none-any.whl (914 kB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 914.4/914.4 kB 415.3 kB/s  0:00:01
Using cached build-1.3.0-py3-none-any.whl (23 kB)
Using cached certifi-2025.10.5-py3-none-any.whl (163 kB)
Using cached click-8.3.0-py3-none-any.whl (107 kB)
Downloading cloudpickle-3.1.1-py3-none-any.whl (20 kB)
Downloading compressed_tensors-0.12.2-py3-none-any.whl (183 kB)
Downloading datasets-4.3.0-py3-none-any.whl (506 kB)
Downloading dill-0.4.0-py3-none-any.whl (119 kB)
Using cached multiprocess-0.70.16-py312-none-any.whl (146 kB)
Using cached pyarrow-22.0.0-cp312-cp312-manylinux_2_28_x86_64.whl (47.7 MB)
Using cached requests-2.32.5-py3-none-any.whl (64 kB)
Using cached charset_normalizer-3.4.4-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (153 kB)
Downloading decord2-2.0.0-cp312-cp312-manylinux_2_28_x86_64.whl (32.3 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 32.3/32.3 MB 3.6 MB/s  0:00:09
Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)
Using cached einops-0.8.1-py3-none-any.whl (64 kB)
Downloading fastapi-0.120.1-py3-none-any.whl (108 kB)
Downloading starlette-0.49.1-py3-none-any.whl (74 kB)
Downloading annotated_doc-0.0.3-py3-none-any.whl (5.5 kB)
Downloading gguf-0.17.1-py3-none-any.whl (96 kB)
Downloading hf_transfer-0.1.9-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.6/3.6 MB 3.5 MB/s  0:00:01
Downloading interegular-0.3.3-py37-none-any.whl (23 kB)
Downloading ipython-9.6.0-py3-none-any.whl (616 kB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 616.2/616.2 kB 3.3 MB/s  0:00:00
Downloading prompt_toolkit-3.0.52-py3-none-any.whl (391 kB)
Downloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.6/1.6 MB 3.5 MB/s  0:00:00
Downloading parso-0.8.5-py2.py3-none-any.whl (106 kB)
Downloading pexpect-4.9.0-py2.py3-none-any.whl (63 kB)
Downloading ptyprocess-0.7.0-py2.py3-none-any.whl (13 kB)
Using cached pygments-2.19.2-py3-none-any.whl (1.2 MB)
Downloading traitlets-5.14.3-py3-none-any.whl (85 kB)
Downloading decorator-5.2.1-py3-none-any.whl (9.2 kB)
Downloading ipython_pygments_lexers-1.1.1-py3-none-any.whl (8.1 kB)
Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)
Using cached markupsafe-3.0.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (22 kB)
Using cached jsonschema-4.25.1-py3-none-any.whl (90 kB)
Using cached jsonschema_specifications-2025.9.1-py3-none-any.whl (18 kB)
Using cached referencing-0.37.0-py3-none-any.whl (26 kB)
Using cached rpds_py-0.28.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (386 kB)
Using cached lark-1.3.1-py3-none-any.whl (113 kB)
Downloading loguru-0.7.3-py3-none-any.whl (61 kB)
Downloading matplotlib_inline-0.2.1-py3-none-any.whl (9.5 kB)
Downloading modelscope-1.31.0-py3-none-any.whl (5.9 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.9/5.9 MB 3.6 MB/s  0:00:01
Downloading msgspec-0.19.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (213 kB)
Downloading nest_asyncio-1.6.0-py3-none-any.whl (5.2 kB)
Using cached networkx-3.5-py3-none-any.whl (2.0 MB)
Using cached ninja-1.13.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (180 kB)
Downloading nvidia_ml_py-13.580.82-py3-none-any.whl (49 kB)
Downloading orjson-3.11.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (136 kB)
Using cached pandas-2.3.3-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (12.4 MB)
Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)
Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)
Using cached six-1.17.0-py2.py3-none-any.whl (11 kB)
Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)
Downloading partial_json_parser-0.2.1.1.post6-py3-none-any.whl (10 kB)
Downloading pillow-12.0.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (7.0 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.0/7.0 MB 3.5 MB/s  0:00:01
Using cached psutil-7.1.2-cp36-abi3-manylinux2010_x86_64.manylinux_2_12_x86_64.manylinux_2_28_x86_64.whl (258 kB)
Downloading py_spy-0.4.1-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (2.8 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.8/2.8 MB 3.5 MB/s  0:00:00
Downloading pybase64-1.4.2-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl (71 kB)
Downloading pycountry-24.6.1-py3-none-any.whl (6.3 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.3/6.3 MB 3.5 MB/s  0:00:01
Using cached pycparser-2.23-py3-none-any.whl (118 kB)
Using cached pyproject_hooks-1.2.0-py3-none-any.whl (10 kB)
Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)
Downloading scipy-1.16.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (35.7 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 35.7/35.7 MB 3.5 MB/s  0:00:10
Using cached sentencepiece-0.2.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (1.4 MB)
Downloading setproctitle-1.3.7-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (32 kB)
Downloading stack_data-0.6.3-py3-none-any.whl (24 kB)
Downloading asttokens-3.0.0-py3-none-any.whl (26 kB)
Downloading executing-2.2.1-py2.py3-none-any.whl (28 kB)
Downloading pure_eval-0.2.3-py3-none-any.whl (11 kB)
Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)
Using cached tiktoken-0.12.0-cp312-cp312-manylinux_2_28_x86_64.whl (1.2 MB)
Downloading torchvision-0.23.0-cp312-cp312-manylinux_2_28_x86_64.whl (8.6 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 8.6/8.6 MB 3.6 MB/s  0:00:02
Using cached uvicorn-0.38.0-py3-none-any.whl (68 kB)
Using cached uvloop-0.22.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (4.4 MB)
Downloading wcwidth-0.2.14-py2.py3-none-any.whl (37 kB)
Using cached xxhash-3.6.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (193 kB)
Building wheels for collected packages: sglang, flashinfer_python
  Building editable for sglang (pyproject.toml) ... done
  Created wheel for sglang: filename=sglang-0.5.4.post1-0.editable-py3-none-any.whl size=4410 sha256=0ff01651c1ba6701df46c5b2dbc65be0bbc09b10f62b05792900bfba1d76ac24
  Stored in directory: /tmp/pip-ephem-wheel-cache-ig94_hha/wheels/3c/42/53/5686d213b6389195a2f57e8b00730c6a0aee0c2670baf8fb72
  Building wheel for flashinfer_python (pyproject.toml) ... done
  Created wheel for flashinfer_python: filename=flashinfer_python-0.4.1-py3-none-any.whl size=6849339 sha256=176a7dc966613e41ddf28ddb566a6b028eabe543f4944d3c97a7f42f59b26c9a
  Stored in directory: /home/m/.cache/pip/wheels/3c/9c/7a/086006f903c0ca6db33268e5225621053eab289d66178cd8a3
Successfully built sglang flashinfer_python
Installing collected packages: torchao, pytz, py-spy, pure-eval, ptyprocess, nvidia-ml-py, nvidia-cusparselt-cu12, mpmath, xxhash, wcwidth, uvloop, urllib3, tzdata, typing-extensions, traitlets, tqdm, torch_memory_saver, tabulate, sympy, sniffio, six, sgl-kernel, setuptools, setproctitle, sentencepiece, safetensors, rpds-py, regex, pyzmq, pyyaml, python-multipart, pyproject_hooks, pygments, pycryptodomex, pycparser, pycountry, pybase64, pyarrow, psutil, protobuf, propcache, prometheus-client, pillow, pexpect, partial_json_parser, parso, packaging, orjson, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cudnn-frontend, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, ninja, networkx, nest_asyncio, multidict, msgspec, MarkupSafe, lxml, loguru, llguidance, lark, jiter, interegular, idna, hf-xet, hf_transfer, h11, fsspec, frozenlist, filelock, executing, einops, docstring-parser, distro, diskcache, dill, decorator, cuda-pathfinder, cloudpickle, click, charset_normalizer, certifi, attrs, asttokens, annotated-types, annotated-doc, airportsdata, aiohappyeyeballs, yarl, uvicorn, typing-inspection, triton, stack_data, scipy, requests, referencing, python-dateutil, pydantic-core, prompt_toolkit, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, multiprocess, matplotlib-inline, jinja2, jedi, ipython-pygments-lexers, httpcore, grpcio, gguf, decord2, cuda-bindings, cffi, build, blobfile, apache-tvm-ffi, anyio, aiosignal, tiktoken, starlette, soundfile, pydantic, pandas, nvidia-cusolver-cu12, modelscope, jsonschema-specifications, IPython, huggingface_hub, httpx, grpcio-tools, grpcio-reflection, grpcio-health-checking, cuda-python, aiohttp, torch, tokenizers, openai-harmony, openai, nvidia-cutlass-dsl, jsonschema, fastapi, anthropic, transformers, torchvision, torchaudio, outlines_core, flashinfer_python, datasets, xgrammar, timm, outlines, compressed-tensors, sglang
Successfully installed IPython-9.6.0 MarkupSafe-3.0.3 aiohappyeyeballs-2.6.1 aiohttp-3.13.1 aiosignal-1.4.0 airportsdata-20250909 annotated-doc-0.0.3 annotated-types-0.7.0 anthropic-0.72.0 anyio-4.11.0 apache-tvm-ffi-0.1.0b15 asttokens-3.0.0 attrs-25.4.0 blobfile-3.0.0 build-1.3.0 certifi-2025.10.5 cffi-2.0.0 charset_normalizer-3.4.4 click-8.3.0 cloudpickle-3.1.1 compressed-tensors-0.12.2 cuda-bindings-13.0.3 cuda-pathfinder-1.3.1 cuda-python-13.0.3 datasets-4.3.0 decorator-5.2.1 decord2-2.0.0 dill-0.4.0 diskcache-5.6.3 distro-1.9.0 docstring-parser-0.17.0 einops-0.8.1 executing-2.2.1 fastapi-0.120.1 filelock-3.20.0 flashinfer_python-0.4.1 frozenlist-1.8.0 fsspec-2025.9.0 gguf-0.17.1 grpcio-1.75.1 grpcio-health-checking-1.75.1 grpcio-reflection-1.75.1 grpcio-tools-1.75.1 h11-0.16.0 hf-xet-1.2.0 hf_transfer-0.1.9 httpcore-1.0.9 httpx-0.28.1 huggingface_hub-0.36.0 idna-3.11 interegular-0.3.3 ipython-pygments-lexers-1.1.1 jedi-0.19.2 jinja2-3.1.6 jiter-0.11.1 jsonschema-4.25.1 jsonschema-specifications-2025.9.1 lark-1.3.1 llguidance-0.7.30 loguru-0.7.3 lxml-6.0.2 matplotlib-inline-0.2.1 modelscope-1.31.0 mpmath-1.3.0 msgspec-0.19.0 multidict-6.7.0 multiprocess-0.70.16 nest_asyncio-1.6.0 networkx-3.5 ninja-1.13.0 numpy-2.3.4 nvidia-cublas-cu12-12.8.4.1 nvidia-cuda-cupti-cu12-12.8.90 nvidia-cuda-nvrtc-cu12-12.8.93 nvidia-cuda-runtime-cu12-12.8.90 nvidia-cudnn-cu12-9.10.2.21 nvidia-cudnn-frontend-1.15.0 nvidia-cufft-cu12-11.3.3.83 nvidia-cufile-cu12-1.13.1.3 nvidia-curand-cu12-10.3.9.90 nvidia-cusolver-cu12-11.7.3.90 nvidia-cusparse-cu12-12.5.8.93 nvidia-cusparselt-cu12-0.7.1 nvidia-cutlass-dsl-4.2.1 nvidia-ml-py-13.580.82 nvidia-nccl-cu12-2.27.3 nvidia-nvjitlink-cu12-12.8.93 nvidia-nvtx-cu12-12.8.90 openai-1.99.1 openai-harmony-0.0.4 orjson-3.11.4 outlines-0.1.11 outlines_core-0.1.26 packaging-25.0 pandas-2.3.3 parso-0.8.5 partial_json_parser-0.2.1.1.post6 pexpect-4.9.0 pillow-12.0.0 prometheus-client-0.23.1 prompt_toolkit-3.0.52 propcache-0.4.1 protobuf-6.33.0 psutil-7.1.2 ptyprocess-0.7.0 pure-eval-0.2.3 py-spy-0.4.1 pyarrow-22.0.0 pybase64-1.4.2 pycountry-24.6.1 pycparser-2.23 pycryptodomex-3.23.0 pydantic-2.12.3 pydantic-core-2.41.4 pygments-2.19.2 pyproject_hooks-1.2.0 python-dateutil-2.9.0.post0 python-multipart-0.0.20 pytz-2025.2 pyyaml-6.0.3 pyzmq-27.1.0 referencing-0.37.0 regex-2025.10.23 requests-2.32.5 rpds-py-0.28.0 safetensors-0.6.2 scipy-1.16.3 sentencepiece-0.2.1 setproctitle-1.3.7 setuptools-80.9.0 sgl-kernel-0.3.16.post4 sglang-0.5.4.post1 six-1.17.0 sniffio-1.3.1 soundfile-0.13.1 stack_data-0.6.3 starlette-0.49.1 sympy-1.14.0 tabulate-0.9.0 tiktoken-0.12.0 timm-1.0.16 tokenizers-0.22.1 torch-2.8.0 torch_memory_saver-0.0.9 torchao-0.9.0 torchaudio-2.8.0 torchvision-0.23.0 tqdm-4.67.1 traitlets-5.14.3 transformers-4.57.1 triton-3.4.0 typing-extensions-4.15.0 typing-inspection-0.4.2 tzdata-2025.2 urllib3-2.5.0 uvicorn-0.38.0 uvloop-0.22.1 wcwidth-0.2.14 xgrammar-0.1.25 xxhash-3.6.0 yarl-1.22.0
(venv) m@m-HP-Z440-Workstation:~/Desktop/sglang/sglang$ /home/m/huggingface/Qwen3-0.6B
bash: /home/m/huggingface/Qwen3-0.6B: Is a directory
(venv) m@m-HP-Z440-Workstation:~/Desktop/sglang/sglang$ python3 -m sglang.launch_server --model-path /home/m/huggingface/Qwen3-0.6B --mem-fraction-static 0.5
[2025-10-28 23:04:16] WARNING server_args.py:1104: Attention backend not explicitly specified. Use flashinfer backend by default.
[2025-10-28 23:04:17] INFO trace.py:48: opentelemetry package is not installed, tracing disabled
[2025-10-28 23:04:17] server_args=ServerArgs(model_path='/home/m/huggingface/Qwen3-0.6B', tokenizer_path='/home/m/huggingface/Qwen3-0.6B', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=False, context_length=None, is_embedding=False, enable_multimodal=None, revision=None, model_impl='auto', host='127.0.0.1', port=30000, grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, mem_fraction_static=0.5, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=2048, max_prefill_tokens=16384, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=1, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='cuda', tp_size=1, pp_size=1, pp_max_micro_batch_size=None, stream_interval=1, stream_output=False, random_seed=631599858, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, oltp_traces_endpoint='localhost:4317', api_key=None, served_model_name='/home/m/huggingface/Qwen3-0.6B', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='triton', max_lora_chunk_size=16, attention_backend='flashinfer', decode_attention_backend=None, prefill_attention_backend=None, sampling_backend='flashinfer', grammar_backend='xgrammar', mm_attention_backend=None, nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm='static', init_expert_location='trivial', enable_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_amx_weight_path=None, kt_amx_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=8, cuda_graph_bs=[1, 2, 4, 8], disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8)
[2025-10-28 23:04:18] Using default HuggingFace chat template with detected content format: string
[2025-10-28 23:04:25] INFO trace.py:48: opentelemetry package is not installed, tracing disabled
[2025-10-28 23:04:25] INFO trace.py:48: opentelemetry package is not installed, tracing disabled
[2025-10-28 23:04:26] Init torch distributed begin.
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[2025-10-28 23:04:26] Init torch distributed ends. mem usage=0.00 GB
[2025-10-28 23:04:26] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2025-10-28 23:04:28] Load weight begin. avail mem=15.08 GB
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:04<00:00,  4.11s/it]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:04<00:00,  4.11s/it]

[2025-10-28 23:04:32] Load weight end. type=Qwen3ForCausalLM, dtype=torch.bfloat16, avail mem=13.82 GB, mem usage=1.27 GB.
[2025-10-28 23:04:32] Using KV cache dtype: torch.bfloat16
[2025-10-28 23:04:32] KV Cache is allocated. #tokens: 58737, K size: 3.14 GB, V size: 3.14 GB
[2025-10-28 23:04:32] Memory pool end. avail mem=7.16 GB
[2025-10-28 23:04:32] Capture cuda graph begin. This can take up to several minutes. avail mem=6.53 GB
[2025-10-28 23:04:32] Capture cuda graph bs [1, 2, 4, 8]
Capturing batches (bs=8 avail_mem=6.53 GB):   0%|                                                  | 0/4 [00:01<?, ?it/s]
[2025-10-28 23:04:34] Scheduler hit an exception: Traceback (most recent call last):
  File "/home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/jit/cpp_ext.py", line 282, in run_ninja
    subprocess.run(
  File "/usr/lib/python3.12/subprocess.py", line 571, in run
    raise CalledProcessError(retcode, process.args,
subprocess.CalledProcessError: Command '['ninja', '-v', '-C', '/home/m/.cache/flashinfer/0.4.1/120a/cached_ops', '-f', '/home/m/.cache/flashinfer/0.4.1/120a/cached_ops/batch_decode_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False/build.ninja']' returned non-zero exit status 1.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/m/Desktop/sglang/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py", line 378, in __init__
    self.capture()
  File "/home/m/Desktop/sglang/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py", line 497, in capture
    ) = self.capture_one_batch_size(bs, forward)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/Desktop/sglang/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py", line 637, in capture_one_batch_size
    self.model_runner.attn_backend.init_forward_metadata_capture_cuda_graph(
  File "/home/m/Desktop/sglang/sglang/python/sglang/srt/layers/attention/flashinfer_backend.py", line 559, in init_forward_metadata_capture_cuda_graph
    self.indices_updater_decode.update(
  File "/home/m/Desktop/sglang/sglang/python/sglang/srt/layers/attention/flashinfer_backend.py", line 902, in update_single_wrapper
    self.call_begin_forward(
  File "/home/m/Desktop/sglang/sglang/python/sglang/srt/layers/attention/flashinfer_backend.py", line 1083, in call_begin_forward
    wrapper.begin_forward(
  File "/home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/decode.py", line 1068, in plan
    self._cached_module = get_batch_decode_module(
                          ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/decode.py", line 211, in get_batch_decode_module
    mod = gen_batch_decode_module(*args).build_and_load()
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/jit/core.py", line 277, in build_and_load
    self.build(verbose, need_lock=False)
  File "/home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/jit/core.py", line 263, in build
    run_ninja(jit_env.FLASHINFER_JIT_DIR, self.ninja_path, verbose)
  File "/home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/jit/cpp_ext.py", line 294, in run_ninja
    raise RuntimeError(msg) from e
RuntimeError: Ninja build failed. Ninja output:
ninja: Entering directory `/home/m/.cache/flashinfer/0.4.1/120a/cached_ops'
[1/4] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output batch_decode_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False/batch_decode_kernel.cuda.o.d -DPy_LIMITED_API=0x03090000 -D_GLIBCXX_USE_CXX11_ABI=1 -isystem /usr/include/python3.12 -isystem /usr/local/cuda/include -isystem /usr/local/cuda/include/cccl -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/tvm_ffi/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/tvm_ffi/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/csrc -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/cutlass/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/cutlass/tools/util/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/spdlog/include --compiler-options=-fPIC --expt-relaxed-constexpr -gencode=arch=compute_120a,code=sm_120a -DFLASHINFER_ENABLE_FP8_E8M0 -DFLASHINFER_ENABLE_FP4_E2M1 -std=c++17 --threads=1 -use_fast_math -DFLASHINFER_ENABLE_F16 -DFLASHINFER_ENABLE_BF16 -DFLASHINFER_ENABLE_FP8_E4M3 -DFLASHINFER_ENABLE_FP8_E5M2 -DNDEBUG -O3 -c /home/m/.cache/flashinfer/0.4.1/120a/generated/batch_decode_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False/batch_decode_kernel.cu -o batch_decode_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False/batch_decode_kernel.cuda.o 
FAILED: [code=1] batch_decode_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False/batch_decode_kernel.cuda.o 
/usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output batch_decode_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False/batch_decode_kernel.cuda.o.d -DPy_LIMITED_API=0x03090000 -D_GLIBCXX_USE_CXX11_ABI=1 -isystem /usr/include/python3.12 -isystem /usr/local/cuda/include -isystem /usr/local/cuda/include/cccl -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/tvm_ffi/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/tvm_ffi/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/csrc -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/cutlass/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/cutlass/tools/util/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/spdlog/include --compiler-options=-fPIC --expt-relaxed-constexpr -gencode=arch=compute_120a,code=sm_120a -DFLASHINFER_ENABLE_FP8_E8M0 -DFLASHINFER_ENABLE_FP4_E2M1 -std=c++17 --threads=1 -use_fast_math -DFLASHINFER_ENABLE_F16 -DFLASHINFER_ENABLE_BF16 -DFLASHINFER_ENABLE_FP8_E4M3 -DFLASHINFER_ENABLE_FP8_E5M2 -DNDEBUG -O3 -c /home/m/.cache/flashinfer/0.4.1/120a/generated/batch_decode_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False/batch_decode_kernel.cu -o batch_decode_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False/batch_decode_kernel.cuda.o 
nvcc fatal   : Unsupported gpu architecture 'compute_120a'
[2/4] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output batch_decode_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False/batch_decode_jit_binding.cuda.o.d -DPy_LIMITED_API=0x03090000 -D_GLIBCXX_USE_CXX11_ABI=1 -isystem /usr/include/python3.12 -isystem /usr/local/cuda/include -isystem /usr/local/cuda/include/cccl -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/tvm_ffi/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/tvm_ffi/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/csrc -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/cutlass/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/cutlass/tools/util/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/spdlog/include --compiler-options=-fPIC --expt-relaxed-constexpr -gencode=arch=compute_120a,code=sm_120a -DFLASHINFER_ENABLE_FP8_E8M0 -DFLASHINFER_ENABLE_FP4_E2M1 -std=c++17 --threads=1 -use_fast_math -DFLASHINFER_ENABLE_F16 -DFLASHINFER_ENABLE_BF16 -DFLASHINFER_ENABLE_FP8_E4M3 -DFLASHINFER_ENABLE_FP8_E5M2 -DNDEBUG -O3 -c /home/m/.cache/flashinfer/0.4.1/120a/generated/batch_decode_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False/batch_decode_jit_binding.cu -o batch_decode_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False/batch_decode_jit_binding.cuda.o 
FAILED: [code=1] batch_decode_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False/batch_decode_jit_binding.cuda.o 
/usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output batch_decode_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False/batch_decode_jit_binding.cuda.o.d -DPy_LIMITED_API=0x03090000 -D_GLIBCXX_USE_CXX11_ABI=1 -isystem /usr/include/python3.12 -isystem /usr/local/cuda/include -isystem /usr/local/cuda/include/cccl -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/tvm_ffi/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/tvm_ffi/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/csrc -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/cutlass/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/cutlass/tools/util/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/spdlog/include --compiler-options=-fPIC --expt-relaxed-constexpr -gencode=arch=compute_120a,code=sm_120a -DFLASHINFER_ENABLE_FP8_E8M0 -DFLASHINFER_ENABLE_FP4_E2M1 -std=c++17 --threads=1 -use_fast_math -DFLASHINFER_ENABLE_F16 -DFLASHINFER_ENABLE_BF16 -DFLASHINFER_ENABLE_FP8_E4M3 -DFLASHINFER_ENABLE_FP8_E5M2 -DNDEBUG -O3 -c /home/m/.cache/flashinfer/0.4.1/120a/generated/batch_decode_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False/batch_decode_jit_binding.cu -o batch_decode_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False/batch_decode_jit_binding.cuda.o 
nvcc fatal   : Unsupported gpu architecture 'compute_120a'
[3/4] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output batch_decode_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False/batch_decode.cuda.o.d -DPy_LIMITED_API=0x03090000 -D_GLIBCXX_USE_CXX11_ABI=1 -isystem /usr/include/python3.12 -isystem /usr/local/cuda/include -isystem /usr/local/cuda/include/cccl -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/tvm_ffi/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/tvm_ffi/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/csrc -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/cutlass/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/cutlass/tools/util/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/spdlog/include --compiler-options=-fPIC --expt-relaxed-constexpr -gencode=arch=compute_120a,code=sm_120a -DFLASHINFER_ENABLE_FP8_E8M0 -DFLASHINFER_ENABLE_FP4_E2M1 -std=c++17 --threads=1 -use_fast_math -DFLASHINFER_ENABLE_F16 -DFLASHINFER_ENABLE_BF16 -DFLASHINFER_ENABLE_FP8_E4M3 -DFLASHINFER_ENABLE_FP8_E5M2 -DNDEBUG -O3 -c /home/m/.cache/flashinfer/0.4.1/120a/generated/batch_decode_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False/batch_decode.cu -o batch_decode_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False/batch_decode.cuda.o 
FAILED: [code=1] batch_decode_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False/batch_decode.cuda.o 
/usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output batch_decode_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False/batch_decode.cuda.o.d -DPy_LIMITED_API=0x03090000 -D_GLIBCXX_USE_CXX11_ABI=1 -isystem /usr/include/python3.12 -isystem /usr/local/cuda/include -isystem /usr/local/cuda/include/cccl -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/tvm_ffi/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/tvm_ffi/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/csrc -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/cutlass/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/cutlass/tools/util/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/spdlog/include --compiler-options=-fPIC --expt-relaxed-constexpr -gencode=arch=compute_120a,code=sm_120a -DFLASHINFER_ENABLE_FP8_E8M0 -DFLASHINFER_ENABLE_FP4_E2M1 -std=c++17 --threads=1 -use_fast_math -DFLASHINFER_ENABLE_F16 -DFLASHINFER_ENABLE_BF16 -DFLASHINFER_ENABLE_FP8_E4M3 -DFLASHINFER_ENABLE_FP8_E5M2 -DNDEBUG -O3 -c /home/m/.cache/flashinfer/0.4.1/120a/generated/batch_decode_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False/batch_decode.cu -o batch_decode_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False/batch_decode.cuda.o 
nvcc fatal   : Unsupported gpu architecture 'compute_120a'
ninja: build stopped: subcommand failed.


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/m/Desktop/sglang/sglang/python/sglang/srt/managers/scheduler.py", line 2753, in run_scheduler_process
    scheduler = Scheduler(
                ^^^^^^^^^^
  File "/home/m/Desktop/sglang/sglang/python/sglang/srt/managers/scheduler.py", line 311, in __init__
    self.tp_worker = TpModelWorker(
                     ^^^^^^^^^^^^^^
  File "/home/m/Desktop/sglang/sglang/python/sglang/srt/managers/tp_worker.py", line 235, in __init__
    self._model_runner = ModelRunner(
                         ^^^^^^^^^^^^
  File "/home/m/Desktop/sglang/sglang/python/sglang/srt/model_executor/model_runner.py", line 312, in __init__
    self.initialize(min_per_gpu_memory)
  File "/home/m/Desktop/sglang/sglang/python/sglang/srt/model_executor/model_runner.py", line 465, in initialize
    self.init_device_graphs()
  File "/home/m/Desktop/sglang/sglang/python/sglang/srt/model_executor/model_runner.py", line 1916, in init_device_graphs
    self.graph_runner = graph_runners[self.device](self)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/Desktop/sglang/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py", line 380, in __init__
    raise Exception(
Exception: Capture cuda graph failed: Ninja build failed. Ninja output:
ninja: Entering directory `/home/m/.cache/flashinfer/0.4.1/120a/cached_ops'
[1/4] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output batch_decode_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False/batch_decode_kernel.cuda.o.d -DPy_LIMITED_API=0x03090000 -D_GLIBCXX_USE_CXX11_ABI=1 -isystem /usr/include/python3.12 -isystem /usr/local/cuda/include -isystem /usr/local/cuda/include/cccl -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/tvm_ffi/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/tvm_ffi/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/csrc -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/cutlass/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/cutlass/tools/util/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/spdlog/include --compiler-options=-fPIC --expt-relaxed-constexpr -gencode=arch=compute_120a,code=sm_120a -DFLASHINFER_ENABLE_FP8_E8M0 -DFLASHINFER_ENABLE_FP4_E2M1 -std=c++17 --threads=1 -use_fast_math -DFLASHINFER_ENABLE_F16 -DFLASHINFER_ENABLE_BF16 -DFLASHINFER_ENABLE_FP8_E4M3 -DFLASHINFER_ENABLE_FP8_E5M2 -DNDEBUG -O3 -c /home/m/.cache/flashinfer/0.4.1/120a/generated/batch_decode_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False/batch_decode_kernel.cu -o batch_decode_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False/batch_decode_kernel.cuda.o 
FAILED: [code=1] batch_decode_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False/batch_decode_kernel.cuda.o 
/usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output batch_decode_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False/batch_decode_kernel.cuda.o.d -DPy_LIMITED_API=0x03090000 -D_GLIBCXX_USE_CXX11_ABI=1 -isystem /usr/include/python3.12 -isystem /usr/local/cuda/include -isystem /usr/local/cuda/include/cccl -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/tvm_ffi/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/tvm_ffi/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/csrc -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/cutlass/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/cutlass/tools/util/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/spdlog/include --compiler-options=-fPIC --expt-relaxed-constexpr -gencode=arch=compute_120a,code=sm_120a -DFLASHINFER_ENABLE_FP8_E8M0 -DFLASHINFER_ENABLE_FP4_E2M1 -std=c++17 --threads=1 -use_fast_math -DFLASHINFER_ENABLE_F16 -DFLASHINFER_ENABLE_BF16 -DFLASHINFER_ENABLE_FP8_E4M3 -DFLASHINFER_ENABLE_FP8_E5M2 -DNDEBUG -O3 -c /home/m/.cache/flashinfer/0.4.1/120a/generated/batch_decode_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False/batch_decode_kernel.cu -o batch_decode_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False/batch_decode_kernel.cuda.o 
nvcc fatal   : Unsupported gpu architecture 'compute_120a'
[2/4] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output batch_decode_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False/batch_decode_jit_binding.cuda.o.d -DPy_LIMITED_API=0x03090000 -D_GLIBCXX_USE_CXX11_ABI=1 -isystem /usr/include/python3.12 -isystem /usr/local/cuda/include -isystem /usr/local/cuda/include/cccl -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/tvm_ffi/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/tvm_ffi/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/csrc -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/cutlass/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/cutlass/tools/util/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/spdlog/include --compiler-options=-fPIC --expt-relaxed-constexpr -gencode=arch=compute_120a,code=sm_120a -DFLASHINFER_ENABLE_FP8_E8M0 -DFLASHINFER_ENABLE_FP4_E2M1 -std=c++17 --threads=1 -use_fast_math -DFLASHINFER_ENABLE_F16 -DFLASHINFER_ENABLE_BF16 -DFLASHINFER_ENABLE_FP8_E4M3 -DFLASHINFER_ENABLE_FP8_E5M2 -DNDEBUG -O3 -c /home/m/.cache/flashinfer/0.4.1/120a/generated/batch_decode_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False/batch_decode_jit_binding.cu -o batch_decode_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False/batch_decode_jit_binding.cuda.o 
FAILED: [code=1] batch_decode_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False/batch_decode_jit_binding.cuda.o 
/usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output batch_decode_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False/batch_decode_jit_binding.cuda.o.d -DPy_LIMITED_API=0x03090000 -D_GLIBCXX_USE_CXX11_ABI=1 -isystem /usr/include/python3.12 -isystem /usr/local/cuda/include -isystem /usr/local/cuda/include/cccl -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/tvm_ffi/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/tvm_ffi/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/csrc -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/cutlass/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/cutlass/tools/util/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/spdlog/include --compiler-options=-fPIC --expt-relaxed-constexpr -gencode=arch=compute_120a,code=sm_120a -DFLASHINFER_ENABLE_FP8_E8M0 -DFLASHINFER_ENABLE_FP4_E2M1 -std=c++17 --threads=1 -use_fast_math -DFLASHINFER_ENABLE_F16 -DFLASHINFER_ENABLE_BF16 -DFLASHINFER_ENABLE_FP8_E4M3 -DFLASHINFER_ENABLE_FP8_E5M2 -DNDEBUG -O3 -c /home/m/.cache/flashinfer/0.4.1/120a/generated/batch_decode_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False/batch_decode_jit_binding.cu -o batch_decode_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False/batch_decode_jit_binding.cuda.o 
nvcc fatal   : Unsupported gpu architecture 'compute_120a'
[3/4] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output batch_decode_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False/batch_decode.cuda.o.d -DPy_LIMITED_API=0x03090000 -D_GLIBCXX_USE_CXX11_ABI=1 -isystem /usr/include/python3.12 -isystem /usr/local/cuda/include -isystem /usr/local/cuda/include/cccl -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/tvm_ffi/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/tvm_ffi/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/csrc -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/cutlass/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/cutlass/tools/util/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/spdlog/include --compiler-options=-fPIC --expt-relaxed-constexpr -gencode=arch=compute_120a,code=sm_120a -DFLASHINFER_ENABLE_FP8_E8M0 -DFLASHINFER_ENABLE_FP4_E2M1 -std=c++17 --threads=1 -use_fast_math -DFLASHINFER_ENABLE_F16 -DFLASHINFER_ENABLE_BF16 -DFLASHINFER_ENABLE_FP8_E4M3 -DFLASHINFER_ENABLE_FP8_E5M2 -DNDEBUG -O3 -c /home/m/.cache/flashinfer/0.4.1/120a/generated/batch_decode_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False/batch_decode.cu -o batch_decode_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False/batch_decode.cuda.o 
FAILED: [code=1] batch_decode_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False/batch_decode.cuda.o 
/usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output batch_decode_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False/batch_decode.cuda.o.d -DPy_LIMITED_API=0x03090000 -D_GLIBCXX_USE_CXX11_ABI=1 -isystem /usr/include/python3.12 -isystem /usr/local/cuda/include -isystem /usr/local/cuda/include/cccl -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/tvm_ffi/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/tvm_ffi/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/csrc -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/cutlass/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/cutlass/tools/util/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/spdlog/include --compiler-options=-fPIC --expt-relaxed-constexpr -gencode=arch=compute_120a,code=sm_120a -DFLASHINFER_ENABLE_FP8_E8M0 -DFLASHINFER_ENABLE_FP4_E2M1 -std=c++17 --threads=1 -use_fast_math -DFLASHINFER_ENABLE_F16 -DFLASHINFER_ENABLE_BF16 -DFLASHINFER_ENABLE_FP8_E4M3 -DFLASHINFER_ENABLE_FP8_E5M2 -DNDEBUG -O3 -c /home/m/.cache/flashinfer/0.4.1/120a/generated/batch_decode_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False/batch_decode.cu -o batch_decode_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False/batch_decode.cuda.o 
nvcc fatal   : Unsupported gpu architecture 'compute_120a'
ninja: build stopped: subcommand failed.

Possible solutions:
1. set --mem-fraction-static to a smaller value (e.g., 0.8 or 0.7)
2. set --cuda-graph-max-bs to a smaller value (e.g., 16)
3. disable torch compile by not using --enable-torch-compile
4. disable CUDA graph by --disable-cuda-graph. (Not recommended. Huge performance loss)
Open an issue on GitHub https://github.com/sgl-project/sglang/issues/new/choose 


[2025-10-28 23:04:34] Received sigquit from a child process. It usually means the child failed.
Killed
(venv) m@m-HP-Z440-Workstation:~/Desktop/sglang/sglang$ python3 -m sglang.launch_server --model-path /home/m/huggingface/Qwen3-0.6B --mem-fraction-static 0.8
[2025-10-28 23:06:32] WARNING server_args.py:1104: Attention backend not explicitly specified. Use flashinfer backend by default.
[2025-10-28 23:06:32] INFO trace.py:48: opentelemetry package is not installed, tracing disabled
[2025-10-28 23:06:32] server_args=ServerArgs(model_path='/home/m/huggingface/Qwen3-0.6B', tokenizer_path='/home/m/huggingface/Qwen3-0.6B', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=False, context_length=None, is_embedding=False, enable_multimodal=None, revision=None, model_impl='auto', host='127.0.0.1', port=30000, grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, mem_fraction_static=0.8, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=2048, max_prefill_tokens=16384, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=1, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='cuda', tp_size=1, pp_size=1, pp_max_micro_batch_size=None, stream_interval=1, stream_output=False, random_seed=300772745, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, oltp_traces_endpoint='localhost:4317', api_key=None, served_model_name='/home/m/huggingface/Qwen3-0.6B', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='triton', max_lora_chunk_size=16, attention_backend='flashinfer', decode_attention_backend=None, prefill_attention_backend=None, sampling_backend='flashinfer', grammar_backend='xgrammar', mm_attention_backend=None, nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm='static', init_expert_location='trivial', enable_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_amx_weight_path=None, kt_amx_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=8, cuda_graph_bs=[1, 2, 4, 8], disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8)
[2025-10-28 23:06:32] Using default HuggingFace chat template with detected content format: string
[2025-10-28 23:06:40] INFO trace.py:48: opentelemetry package is not installed, tracing disabled
[2025-10-28 23:06:40] INFO trace.py:48: opentelemetry package is not installed, tracing disabled
[2025-10-28 23:06:41] Init torch distributed begin.
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[2025-10-28 23:06:41] Init torch distributed ends. mem usage=0.00 GB
[2025-10-28 23:06:41] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2025-10-28 23:06:42] Load weight begin. avail mem=15.08 GB
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.60it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.59it/s]

[2025-10-28 23:06:42] Load weight end. type=Qwen3ForCausalLM, dtype=torch.bfloat16, avail mem=13.80 GB, mem usage=1.27 GB.
[2025-10-28 23:06:42] Using KV cache dtype: torch.bfloat16
[2025-10-28 23:06:42] KV Cache is allocated. #tokens: 101012, K size: 5.39 GB, V size: 5.39 GB
[2025-10-28 23:06:42] Memory pool end. avail mem=2.66 GB
[2025-10-28 23:06:43] Capture cuda graph begin. This can take up to several minutes. avail mem=2.03 GB
[2025-10-28 23:06:43] Capture cuda graph bs [1, 2, 4, 8]
Capturing batches (bs=8 avail_mem=2.03 GB):   0%|                                                  | 0/4 [00:00<?, ?it/s]
[2025-10-28 23:06:44] Scheduler hit an exception: Traceback (most recent call last):
  File "/home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/jit/cpp_ext.py", line 282, in run_ninja
    subprocess.run(
  File "/usr/lib/python3.12/subprocess.py", line 571, in run
    raise CalledProcessError(retcode, process.args,
subprocess.CalledProcessError: Command '['ninja', '-v', '-C', '/home/m/.cache/flashinfer/0.4.1/120a/cached_ops', '-f', '/home/m/.cache/flashinfer/0.4.1/120a/cached_ops/batch_decode_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False/build.ninja']' returned non-zero exit status 1.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/m/Desktop/sglang/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py", line 378, in __init__
    self.capture()
  File "/home/m/Desktop/sglang/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py", line 497, in capture
    ) = self.capture_one_batch_size(bs, forward)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/Desktop/sglang/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py", line 637, in capture_one_batch_size
    self.model_runner.attn_backend.init_forward_metadata_capture_cuda_graph(
  File "/home/m/Desktop/sglang/sglang/python/sglang/srt/layers/attention/flashinfer_backend.py", line 559, in init_forward_metadata_capture_cuda_graph
    self.indices_updater_decode.update(
  File "/home/m/Desktop/sglang/sglang/python/sglang/srt/layers/attention/flashinfer_backend.py", line 902, in update_single_wrapper
    self.call_begin_forward(
  File "/home/m/Desktop/sglang/sglang/python/sglang/srt/layers/attention/flashinfer_backend.py", line 1083, in call_begin_forward
    wrapper.begin_forward(
  File "/home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/decode.py", line 1068, in plan
    self._cached_module = get_batch_decode_module(
                          ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/decode.py", line 211, in get_batch_decode_module
    mod = gen_batch_decode_module(*args).build_and_load()
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/jit/core.py", line 277, in build_and_load
    self.build(verbose, need_lock=False)
  File "/home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/jit/core.py", line 263, in build
    run_ninja(jit_env.FLASHINFER_JIT_DIR, self.ninja_path, verbose)
  File "/home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/jit/cpp_ext.py", line 294, in run_ninja
    raise RuntimeError(msg) from e
RuntimeError: Ninja build failed. Ninja output:
ninja: Entering directory `/home/m/.cache/flashinfer/0.4.1/120a/cached_ops'
[1/4] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output batch_decode_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False/batch_decode_kernel.cuda.o.d -DPy_LIMITED_API=0x03090000 -D_GLIBCXX_USE_CXX11_ABI=1 -isystem /usr/include/python3.12 -isystem /usr/local/cuda/include -isystem /usr/local/cuda/include/cccl -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/tvm_ffi/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/tvm_ffi/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/csrc -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/cutlass/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/cutlass/tools/util/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/spdlog/include --compiler-options=-fPIC --expt-relaxed-constexpr -gencode=arch=compute_120a,code=sm_120a -DFLASHINFER_ENABLE_FP8_E8M0 -DFLASHINFER_ENABLE_FP4_E2M1 -std=c++17 --threads=1 -use_fast_math -DFLASHINFER_ENABLE_F16 -DFLASHINFER_ENABLE_BF16 -DFLASHINFER_ENABLE_FP8_E4M3 -DFLASHINFER_ENABLE_FP8_E5M2 -DNDEBUG -O3 -c /home/m/.cache/flashinfer/0.4.1/120a/generated/batch_decode_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False/batch_decode_kernel.cu -o batch_decode_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False/batch_decode_kernel.cuda.o 
FAILED: [code=1] batch_decode_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False/batch_decode_kernel.cuda.o 
/usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output batch_decode_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False/batch_decode_kernel.cuda.o.d -DPy_LIMITED_API=0x03090000 -D_GLIBCXX_USE_CXX11_ABI=1 -isystem /usr/include/python3.12 -isystem /usr/local/cuda/include -isystem /usr/local/cuda/include/cccl -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/tvm_ffi/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/tvm_ffi/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/csrc -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/cutlass/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/cutlass/tools/util/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/spdlog/include --compiler-options=-fPIC --expt-relaxed-constexpr -gencode=arch=compute_120a,code=sm_120a -DFLASHINFER_ENABLE_FP8_E8M0 -DFLASHINFER_ENABLE_FP4_E2M1 -std=c++17 --threads=1 -use_fast_math -DFLASHINFER_ENABLE_F16 -DFLASHINFER_ENABLE_BF16 -DFLASHINFER_ENABLE_FP8_E4M3 -DFLASHINFER_ENABLE_FP8_E5M2 -DNDEBUG -O3 -c /home/m/.cache/flashinfer/0.4.1/120a/generated/batch_decode_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False/batch_decode_kernel.cu -o batch_decode_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False/batch_decode_kernel.cuda.o 
nvcc fatal   : Unsupported gpu architecture 'compute_120a'
[2/4] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output batch_decode_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False/batch_decode.cuda.o.d -DPy_LIMITED_API=0x03090000 -D_GLIBCXX_USE_CXX11_ABI=1 -isystem /usr/include/python3.12 -isystem /usr/local/cuda/include -isystem /usr/local/cuda/include/cccl -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/tvm_ffi/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/tvm_ffi/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/csrc -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/cutlass/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/cutlass/tools/util/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/spdlog/include --compiler-options=-fPIC --expt-relaxed-constexpr -gencode=arch=compute_120a,code=sm_120a -DFLASHINFER_ENABLE_FP8_E8M0 -DFLASHINFER_ENABLE_FP4_E2M1 -std=c++17 --threads=1 -use_fast_math -DFLASHINFER_ENABLE_F16 -DFLASHINFER_ENABLE_BF16 -DFLASHINFER_ENABLE_FP8_E4M3 -DFLASHINFER_ENABLE_FP8_E5M2 -DNDEBUG -O3 -c /home/m/.cache/flashinfer/0.4.1/120a/generated/batch_decode_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False/batch_decode.cu -o batch_decode_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False/batch_decode.cuda.o 
FAILED: [code=1] batch_decode_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False/batch_decode.cuda.o 
/usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output batch_decode_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False/batch_decode.cuda.o.d -DPy_LIMITED_API=0x03090000 -D_GLIBCXX_USE_CXX11_ABI=1 -isystem /usr/include/python3.12 -isystem /usr/local/cuda/include -isystem /usr/local/cuda/include/cccl -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/tvm_ffi/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/tvm_ffi/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/csrc -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/cutlass/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/cutlass/tools/util/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/spdlog/include --compiler-options=-fPIC --expt-relaxed-constexpr -gencode=arch=compute_120a,code=sm_120a -DFLASHINFER_ENABLE_FP8_E8M0 -DFLASHINFER_ENABLE_FP4_E2M1 -std=c++17 --threads=1 -use_fast_math -DFLASHINFER_ENABLE_F16 -DFLASHINFER_ENABLE_BF16 -DFLASHINFER_ENABLE_FP8_E4M3 -DFLASHINFER_ENABLE_FP8_E5M2 -DNDEBUG -O3 -c /home/m/.cache/flashinfer/0.4.1/120a/generated/batch_decode_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False/batch_decode.cu -o batch_decode_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False/batch_decode.cuda.o 
nvcc fatal   : Unsupported gpu architecture 'compute_120a'
[3/4] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output batch_decode_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False/batch_decode_jit_binding.cuda.o.d -DPy_LIMITED_API=0x03090000 -D_GLIBCXX_USE_CXX11_ABI=1 -isystem /usr/include/python3.12 -isystem /usr/local/cuda/include -isystem /usr/local/cuda/include/cccl -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/tvm_ffi/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/tvm_ffi/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/csrc -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/cutlass/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/cutlass/tools/util/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/spdlog/include --compiler-options=-fPIC --expt-relaxed-constexpr -gencode=arch=compute_120a,code=sm_120a -DFLASHINFER_ENABLE_FP8_E8M0 -DFLASHINFER_ENABLE_FP4_E2M1 -std=c++17 --threads=1 -use_fast_math -DFLASHINFER_ENABLE_F16 -DFLASHINFER_ENABLE_BF16 -DFLASHINFER_ENABLE_FP8_E4M3 -DFLASHINFER_ENABLE_FP8_E5M2 -DNDEBUG -O3 -c /home/m/.cache/flashinfer/0.4.1/120a/generated/batch_decode_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False/batch_decode_jit_binding.cu -o batch_decode_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False/batch_decode_jit_binding.cuda.o 
FAILED: [code=1] batch_decode_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False/batch_decode_jit_binding.cuda.o 
/usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output batch_decode_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False/batch_decode_jit_binding.cuda.o.d -DPy_LIMITED_API=0x03090000 -D_GLIBCXX_USE_CXX11_ABI=1 -isystem /usr/include/python3.12 -isystem /usr/local/cuda/include -isystem /usr/local/cuda/include/cccl -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/tvm_ffi/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/tvm_ffi/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/csrc -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/cutlass/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/cutlass/tools/util/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/spdlog/include --compiler-options=-fPIC --expt-relaxed-constexpr -gencode=arch=compute_120a,code=sm_120a -DFLASHINFER_ENABLE_FP8_E8M0 -DFLASHINFER_ENABLE_FP4_E2M1 -std=c++17 --threads=1 -use_fast_math -DFLASHINFER_ENABLE_F16 -DFLASHINFER_ENABLE_BF16 -DFLASHINFER_ENABLE_FP8_E4M3 -DFLASHINFER_ENABLE_FP8_E5M2 -DNDEBUG -O3 -c /home/m/.cache/flashinfer/0.4.1/120a/generated/batch_decode_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False/batch_decode_jit_binding.cu -o batch_decode_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False/batch_decode_jit_binding.cuda.o 
nvcc fatal   : Unsupported gpu architecture 'compute_120a'
ninja: build stopped: subcommand failed.


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/m/Desktop/sglang/sglang/python/sglang/srt/managers/scheduler.py", line 2753, in run_scheduler_process
    scheduler = Scheduler(
                ^^^^^^^^^^
  File "/home/m/Desktop/sglang/sglang/python/sglang/srt/managers/scheduler.py", line 311, in __init__
    self.tp_worker = TpModelWorker(
                     ^^^^^^^^^^^^^^
  File "/home/m/Desktop/sglang/sglang/python/sglang/srt/managers/tp_worker.py", line 235, in __init__
    self._model_runner = ModelRunner(
                         ^^^^^^^^^^^^
  File "/home/m/Desktop/sglang/sglang/python/sglang/srt/model_executor/model_runner.py", line 312, in __init__
    self.initialize(min_per_gpu_memory)
  File "/home/m/Desktop/sglang/sglang/python/sglang/srt/model_executor/model_runner.py", line 465, in initialize
    self.init_device_graphs()
  File "/home/m/Desktop/sglang/sglang/python/sglang/srt/model_executor/model_runner.py", line 1916, in init_device_graphs
    self.graph_runner = graph_runners[self.device](self)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/Desktop/sglang/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py", line 380, in __init__
    raise Exception(
Exception: Capture cuda graph failed: Ninja build failed. Ninja output:
ninja: Entering directory `/home/m/.cache/flashinfer/0.4.1/120a/cached_ops'
[1/4] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output batch_decode_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False/batch_decode_kernel.cuda.o.d -DPy_LIMITED_API=0x03090000 -D_GLIBCXX_USE_CXX11_ABI=1 -isystem /usr/include/python3.12 -isystem /usr/local/cuda/include -isystem /usr/local/cuda/include/cccl -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/tvm_ffi/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/tvm_ffi/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/csrc -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/cutlass/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/cutlass/tools/util/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/spdlog/include --compiler-options=-fPIC --expt-relaxed-constexpr -gencode=arch=compute_120a,code=sm_120a -DFLASHINFER_ENABLE_FP8_E8M0 -DFLASHINFER_ENABLE_FP4_E2M1 -std=c++17 --threads=1 -use_fast_math -DFLASHINFER_ENABLE_F16 -DFLASHINFER_ENABLE_BF16 -DFLASHINFER_ENABLE_FP8_E4M3 -DFLASHINFER_ENABLE_FP8_E5M2 -DNDEBUG -O3 -c /home/m/.cache/flashinfer/0.4.1/120a/generated/batch_decode_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False/batch_decode_kernel.cu -o batch_decode_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False/batch_decode_kernel.cuda.o 
FAILED: [code=1] batch_decode_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False/batch_decode_kernel.cuda.o 
/usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output batch_decode_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False/batch_decode_kernel.cuda.o.d -DPy_LIMITED_API=0x03090000 -D_GLIBCXX_USE_CXX11_ABI=1 -isystem /usr/include/python3.12 -isystem /usr/local/cuda/include -isystem /usr/local/cuda/include/cccl -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/tvm_ffi/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/tvm_ffi/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/csrc -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/cutlass/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/cutlass/tools/util/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/spdlog/include --compiler-options=-fPIC --expt-relaxed-constexpr -gencode=arch=compute_120a,code=sm_120a -DFLASHINFER_ENABLE_FP8_E8M0 -DFLASHINFER_ENABLE_FP4_E2M1 -std=c++17 --threads=1 -use_fast_math -DFLASHINFER_ENABLE_F16 -DFLASHINFER_ENABLE_BF16 -DFLASHINFER_ENABLE_FP8_E4M3 -DFLASHINFER_ENABLE_FP8_E5M2 -DNDEBUG -O3 -c /home/m/.cache/flashinfer/0.4.1/120a/generated/batch_decode_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False/batch_decode_kernel.cu -o batch_decode_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False/batch_decode_kernel.cuda.o 
nvcc fatal   : Unsupported gpu architecture 'compute_120a'
[2/4] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output batch_decode_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False/batch_decode.cuda.o.d -DPy_LIMITED_API=0x03090000 -D_GLIBCXX_USE_CXX11_ABI=1 -isystem /usr/include/python3.12 -isystem /usr/local/cuda/include -isystem /usr/local/cuda/include/cccl -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/tvm_ffi/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/tvm_ffi/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/csrc -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/cutlass/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/cutlass/tools/util/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/spdlog/include --compiler-options=-fPIC --expt-relaxed-constexpr -gencode=arch=compute_120a,code=sm_120a -DFLASHINFER_ENABLE_FP8_E8M0 -DFLASHINFER_ENABLE_FP4_E2M1 -std=c++17 --threads=1 -use_fast_math -DFLASHINFER_ENABLE_F16 -DFLASHINFER_ENABLE_BF16 -DFLASHINFER_ENABLE_FP8_E4M3 -DFLASHINFER_ENABLE_FP8_E5M2 -DNDEBUG -O3 -c /home/m/.cache/flashinfer/0.4.1/120a/generated/batch_decode_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False/batch_decode.cu -o batch_decode_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False/batch_decode.cuda.o 
FAILED: [code=1] batch_decode_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False/batch_decode.cuda.o 
/usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output batch_decode_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False/batch_decode.cuda.o.d -DPy_LIMITED_API=0x03090000 -D_GLIBCXX_USE_CXX11_ABI=1 -isystem /usr/include/python3.12 -isystem /usr/local/cuda/include -isystem /usr/local/cuda/include/cccl -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/tvm_ffi/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/tvm_ffi/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/csrc -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/cutlass/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/cutlass/tools/util/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/spdlog/include --compiler-options=-fPIC --expt-relaxed-constexpr -gencode=arch=compute_120a,code=sm_120a -DFLASHINFER_ENABLE_FP8_E8M0 -DFLASHINFER_ENABLE_FP4_E2M1 -std=c++17 --threads=1 -use_fast_math -DFLASHINFER_ENABLE_F16 -DFLASHINFER_ENABLE_BF16 -DFLASHINFER_ENABLE_FP8_E4M3 -DFLASHINFER_ENABLE_FP8_E5M2 -DNDEBUG -O3 -c /home/m/.cache/flashinfer/0.4.1/120a/generated/batch_decode_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False/batch_decode.cu -o batch_decode_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False/batch_decode.cuda.o 
nvcc fatal   : Unsupported gpu architecture 'compute_120a'
[3/4] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output batch_decode_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False/batch_decode_jit_binding.cuda.o.d -DPy_LIMITED_API=0x03090000 -D_GLIBCXX_USE_CXX11_ABI=1 -isystem /usr/include/python3.12 -isystem /usr/local/cuda/include -isystem /usr/local/cuda/include/cccl -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/tvm_ffi/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/tvm_ffi/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/csrc -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/cutlass/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/cutlass/tools/util/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/spdlog/include --compiler-options=-fPIC --expt-relaxed-constexpr -gencode=arch=compute_120a,code=sm_120a -DFLASHINFER_ENABLE_FP8_E8M0 -DFLASHINFER_ENABLE_FP4_E2M1 -std=c++17 --threads=1 -use_fast_math -DFLASHINFER_ENABLE_F16 -DFLASHINFER_ENABLE_BF16 -DFLASHINFER_ENABLE_FP8_E4M3 -DFLASHINFER_ENABLE_FP8_E5M2 -DNDEBUG -O3 -c /home/m/.cache/flashinfer/0.4.1/120a/generated/batch_decode_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False/batch_decode_jit_binding.cu -o batch_decode_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False/batch_decode_jit_binding.cuda.o 
FAILED: [code=1] batch_decode_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False/batch_decode_jit_binding.cuda.o 
/usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output batch_decode_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False/batch_decode_jit_binding.cuda.o.d -DPy_LIMITED_API=0x03090000 -D_GLIBCXX_USE_CXX11_ABI=1 -isystem /usr/include/python3.12 -isystem /usr/local/cuda/include -isystem /usr/local/cuda/include/cccl -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/tvm_ffi/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/tvm_ffi/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/csrc -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/cutlass/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/cutlass/tools/util/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/spdlog/include --compiler-options=-fPIC --expt-relaxed-constexpr -gencode=arch=compute_120a,code=sm_120a -DFLASHINFER_ENABLE_FP8_E8M0 -DFLASHINFER_ENABLE_FP4_E2M1 -std=c++17 --threads=1 -use_fast_math -DFLASHINFER_ENABLE_F16 -DFLASHINFER_ENABLE_BF16 -DFLASHINFER_ENABLE_FP8_E4M3 -DFLASHINFER_ENABLE_FP8_E5M2 -DNDEBUG -O3 -c /home/m/.cache/flashinfer/0.4.1/120a/generated/batch_decode_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False/batch_decode_jit_binding.cu -o batch_decode_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False/batch_decode_jit_binding.cuda.o 
nvcc fatal   : Unsupported gpu architecture 'compute_120a'
ninja: build stopped: subcommand failed.

Possible solutions:
1. set --mem-fraction-static to a smaller value (e.g., 0.8 or 0.7)
2. set --cuda-graph-max-bs to a smaller value (e.g., 16)
3. disable torch compile by not using --enable-torch-compile
4. disable CUDA graph by --disable-cuda-graph. (Not recommended. Huge performance loss)
Open an issue on GitHub https://github.com/sgl-project/sglang/issues/new/choose 


[2025-10-28 23:06:44] Received sigquit from a child process. It usually means the child failed.
Killed
(venv) m@m-HP-Z440-Workstation:~/Desktop/sglang/sglang$ python
Python 3.12.3 (main, Aug 14 2025, 17:47:21) [GCC 13.3.0] on linux
Type "help", "copyright", "credits" or "license" for more information.
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> import sglang as sgl
>>> llm = sgl.Engine(model_path="/home/m/huggingface/Qwen3-0.6B")
[2025-10-28 23:08:21] WARNING server_args.py:1104: Attention backend not explicitly specified. Use flashinfer backend by default.
[2025-10-28 23:08:21] INFO engine.py:124: server_args=ServerArgs(model_path='/home/m/huggingface/Qwen3-0.6B', tokenizer_path='/home/m/huggingface/Qwen3-0.6B', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=False, context_length=None, is_embedding=False, enable_multimodal=None, revision=None, model_impl='auto', host='127.0.0.1', port=30000, grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, mem_fraction_static=0.771, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=2048, max_prefill_tokens=16384, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=1, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='cuda', tp_size=1, pp_size=1, pp_max_micro_batch_size=None, stream_interval=1, stream_output=False, random_seed=63049438, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, log_level='error', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, oltp_traces_endpoint='localhost:4317', api_key=None, served_model_name='/home/m/huggingface/Qwen3-0.6B', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='triton', max_lora_chunk_size=16, attention_backend='flashinfer', decode_attention_backend=None, prefill_attention_backend=None, sampling_backend='flashinfer', grammar_backend='xgrammar', mm_attention_backend=None, nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm='static', init_expert_location='trivial', enable_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_amx_weight_path=None, kt_amx_method=None, kt_cpuinfer=None, kt_threadpool_count=None, kt_num_gpu_experts=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=8, cuda_graph_bs=[1, 2, 4, 8], disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8)
[2025-10-28 23:08:29] INFO trace.py:48: opentelemetry package is not installed, tracing disabled
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.61it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.60it/s]

Capturing batches (bs=8 avail_mem=2.50 GB):   0%|                                                  | 0/4 [00:00<?, ?it/s]
[2025-10-28 23:08:33] Scheduler hit an exception: Traceback (most recent call last):
  File "/home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/jit/cpp_ext.py", line 282, in run_ninja
    subprocess.run(
  File "/usr/lib/python3.12/subprocess.py", line 571, in run
    raise CalledProcessError(retcode, process.args,
subprocess.CalledProcessError: Command '['ninja', '-v', '-C', '/home/m/.cache/flashinfer/0.4.1/120a/cached_ops', '-f', '/home/m/.cache/flashinfer/0.4.1/120a/cached_ops/batch_decode_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False/build.ninja']' returned non-zero exit status 1.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/m/Desktop/sglang/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py", line 378, in __init__
    self.capture()
  File "/home/m/Desktop/sglang/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py", line 497, in capture
    ) = self.capture_one_batch_size(bs, forward)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/Desktop/sglang/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py", line 637, in capture_one_batch_size
    self.model_runner.attn_backend.init_forward_metadata_capture_cuda_graph(
  File "/home/m/Desktop/sglang/sglang/python/sglang/srt/layers/attention/flashinfer_backend.py", line 559, in init_forward_metadata_capture_cuda_graph
    self.indices_updater_decode.update(
  File "/home/m/Desktop/sglang/sglang/python/sglang/srt/layers/attention/flashinfer_backend.py", line 902, in update_single_wrapper
    self.call_begin_forward(
  File "/home/m/Desktop/sglang/sglang/python/sglang/srt/layers/attention/flashinfer_backend.py", line 1083, in call_begin_forward
    wrapper.begin_forward(
  File "/home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/decode.py", line 1068, in plan
    self._cached_module = get_batch_decode_module(
                          ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/decode.py", line 211, in get_batch_decode_module
    mod = gen_batch_decode_module(*args).build_and_load()
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/jit/core.py", line 277, in build_and_load
    self.build(verbose, need_lock=False)
  File "/home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/jit/core.py", line 263, in build
    run_ninja(jit_env.FLASHINFER_JIT_DIR, self.ninja_path, verbose)
  File "/home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/jit/cpp_ext.py", line 294, in run_ninja
    raise RuntimeError(msg) from e
RuntimeError: Ninja build failed. Ninja output:
ninja: Entering directory `/home/m/.cache/flashinfer/0.4.1/120a/cached_ops'
[1/4] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output batch_decode_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False/batch_decode.cuda.o.d -DPy_LIMITED_API=0x03090000 -D_GLIBCXX_USE_CXX11_ABI=1 -isystem /usr/include/python3.12 -isystem /usr/local/cuda/include -isystem /usr/local/cuda/include/cccl -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/tvm_ffi/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/tvm_ffi/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/csrc -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/cutlass/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/cutlass/tools/util/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/spdlog/include --compiler-options=-fPIC --expt-relaxed-constexpr -gencode=arch=compute_120a,code=sm_120a -DFLASHINFER_ENABLE_FP8_E8M0 -DFLASHINFER_ENABLE_FP4_E2M1 -std=c++17 --threads=1 -use_fast_math -DFLASHINFER_ENABLE_F16 -DFLASHINFER_ENABLE_BF16 -DFLASHINFER_ENABLE_FP8_E4M3 -DFLASHINFER_ENABLE_FP8_E5M2 -DNDEBUG -O3 -c /home/m/.cache/flashinfer/0.4.1/120a/generated/batch_decode_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False/batch_decode.cu -o batch_decode_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False/batch_decode.cuda.o 
FAILED: [code=1] batch_decode_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False/batch_decode.cuda.o 
/usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output batch_decode_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False/batch_decode.cuda.o.d -DPy_LIMITED_API=0x03090000 -D_GLIBCXX_USE_CXX11_ABI=1 -isystem /usr/include/python3.12 -isystem /usr/local/cuda/include -isystem /usr/local/cuda/include/cccl -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/tvm_ffi/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/tvm_ffi/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/csrc -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/cutlass/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/cutlass/tools/util/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/spdlog/include --compiler-options=-fPIC --expt-relaxed-constexpr -gencode=arch=compute_120a,code=sm_120a -DFLASHINFER_ENABLE_FP8_E8M0 -DFLASHINFER_ENABLE_FP4_E2M1 -std=c++17 --threads=1 -use_fast_math -DFLASHINFER_ENABLE_F16 -DFLASHINFER_ENABLE_BF16 -DFLASHINFER_ENABLE_FP8_E4M3 -DFLASHINFER_ENABLE_FP8_E5M2 -DNDEBUG -O3 -c /home/m/.cache/flashinfer/0.4.1/120a/generated/batch_decode_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False/batch_decode.cu -o batch_decode_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False/batch_decode.cuda.o 
nvcc fatal   : Unsupported gpu architecture 'compute_120a'
[2/4] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output batch_decode_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False/batch_decode_kernel.cuda.o.d -DPy_LIMITED_API=0x03090000 -D_GLIBCXX_USE_CXX11_ABI=1 -isystem /usr/include/python3.12 -isystem /usr/local/cuda/include -isystem /usr/local/cuda/include/cccl -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/tvm_ffi/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/tvm_ffi/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/csrc -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/cutlass/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/cutlass/tools/util/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/spdlog/include --compiler-options=-fPIC --expt-relaxed-constexpr -gencode=arch=compute_120a,code=sm_120a -DFLASHINFER_ENABLE_FP8_E8M0 -DFLASHINFER_ENABLE_FP4_E2M1 -std=c++17 --threads=1 -use_fast_math -DFLASHINFER_ENABLE_F16 -DFLASHINFER_ENABLE_BF16 -DFLASHINFER_ENABLE_FP8_E4M3 -DFLASHINFER_ENABLE_FP8_E5M2 -DNDEBUG -O3 -c /home/m/.cache/flashinfer/0.4.1/120a/generated/batch_decode_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False/batch_decode_kernel.cu -o batch_decode_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False/batch_decode_kernel.cuda.o 
FAILED: [code=1] batch_decode_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False/batch_decode_kernel.cuda.o 
/usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output batch_decode_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False/batch_decode_kernel.cuda.o.d -DPy_LIMITED_API=0x03090000 -D_GLIBCXX_USE_CXX11_ABI=1 -isystem /usr/include/python3.12 -isystem /usr/local/cuda/include -isystem /usr/local/cuda/include/cccl -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/tvm_ffi/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/tvm_ffi/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/csrc -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/cutlass/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/cutlass/tools/util/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/spdlog/include --compiler-options=-fPIC --expt-relaxed-constexpr -gencode=arch=compute_120a,code=sm_120a -DFLASHINFER_ENABLE_FP8_E8M0 -DFLASHINFER_ENABLE_FP4_E2M1 -std=c++17 --threads=1 -use_fast_math -DFLASHINFER_ENABLE_F16 -DFLASHINFER_ENABLE_BF16 -DFLASHINFER_ENABLE_FP8_E4M3 -DFLASHINFER_ENABLE_FP8_E5M2 -DNDEBUG -O3 -c /home/m/.cache/flashinfer/0.4.1/120a/generated/batch_decode_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False/batch_decode_kernel.cu -o batch_decode_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False/batch_decode_kernel.cuda.o 
nvcc fatal   : Unsupported gpu architecture 'compute_120a'
[3/4] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output batch_decode_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False/batch_decode_jit_binding.cuda.o.d -DPy_LIMITED_API=0x03090000 -D_GLIBCXX_USE_CXX11_ABI=1 -isystem /usr/include/python3.12 -isystem /usr/local/cuda/include -isystem /usr/local/cuda/include/cccl -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/tvm_ffi/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/tvm_ffi/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/csrc -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/cutlass/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/cutlass/tools/util/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/spdlog/include --compiler-options=-fPIC --expt-relaxed-constexpr -gencode=arch=compute_120a,code=sm_120a -DFLASHINFER_ENABLE_FP8_E8M0 -DFLASHINFER_ENABLE_FP4_E2M1 -std=c++17 --threads=1 -use_fast_math -DFLASHINFER_ENABLE_F16 -DFLASHINFER_ENABLE_BF16 -DFLASHINFER_ENABLE_FP8_E4M3 -DFLASHINFER_ENABLE_FP8_E5M2 -DNDEBUG -O3 -c /home/m/.cache/flashinfer/0.4.1/120a/generated/batch_decode_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False/batch_decode_jit_binding.cu -o batch_decode_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False/batch_decode_jit_binding.cuda.o 
FAILED: [code=1] batch_decode_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False/batch_decode_jit_binding.cuda.o 
/usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output batch_decode_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False/batch_decode_jit_binding.cuda.o.d -DPy_LIMITED_API=0x03090000 -D_GLIBCXX_USE_CXX11_ABI=1 -isystem /usr/include/python3.12 -isystem /usr/local/cuda/include -isystem /usr/local/cuda/include/cccl -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/tvm_ffi/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/tvm_ffi/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/csrc -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/cutlass/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/cutlass/tools/util/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/spdlog/include --compiler-options=-fPIC --expt-relaxed-constexpr -gencode=arch=compute_120a,code=sm_120a -DFLASHINFER_ENABLE_FP8_E8M0 -DFLASHINFER_ENABLE_FP4_E2M1 -std=c++17 --threads=1 -use_fast_math -DFLASHINFER_ENABLE_F16 -DFLASHINFER_ENABLE_BF16 -DFLASHINFER_ENABLE_FP8_E4M3 -DFLASHINFER_ENABLE_FP8_E5M2 -DNDEBUG -O3 -c /home/m/.cache/flashinfer/0.4.1/120a/generated/batch_decode_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False/batch_decode_jit_binding.cu -o batch_decode_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False/batch_decode_jit_binding.cuda.o 
nvcc fatal   : Unsupported gpu architecture 'compute_120a'
ninja: build stopped: subcommand failed.


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/m/Desktop/sglang/sglang/python/sglang/srt/managers/scheduler.py", line 2753, in run_scheduler_process
    scheduler = Scheduler(
                ^^^^^^^^^^
  File "/home/m/Desktop/sglang/sglang/python/sglang/srt/managers/scheduler.py", line 311, in __init__
    self.tp_worker = TpModelWorker(
                     ^^^^^^^^^^^^^^
  File "/home/m/Desktop/sglang/sglang/python/sglang/srt/managers/tp_worker.py", line 235, in __init__
    self._model_runner = ModelRunner(
                         ^^^^^^^^^^^^
  File "/home/m/Desktop/sglang/sglang/python/sglang/srt/model_executor/model_runner.py", line 312, in __init__
    self.initialize(min_per_gpu_memory)
  File "/home/m/Desktop/sglang/sglang/python/sglang/srt/model_executor/model_runner.py", line 465, in initialize
    self.init_device_graphs()
  File "/home/m/Desktop/sglang/sglang/python/sglang/srt/model_executor/model_runner.py", line 1916, in init_device_graphs
    self.graph_runner = graph_runners[self.device](self)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/Desktop/sglang/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py", line 380, in __init__
    raise Exception(
Exception: Capture cuda graph failed: Ninja build failed. Ninja output:
ninja: Entering directory `/home/m/.cache/flashinfer/0.4.1/120a/cached_ops'
[1/4] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output batch_decode_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False/batch_decode.cuda.o.d -DPy_LIMITED_API=0x03090000 -D_GLIBCXX_USE_CXX11_ABI=1 -isystem /usr/include/python3.12 -isystem /usr/local/cuda/include -isystem /usr/local/cuda/include/cccl -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/tvm_ffi/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/tvm_ffi/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/csrc -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/cutlass/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/cutlass/tools/util/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/spdlog/include --compiler-options=-fPIC --expt-relaxed-constexpr -gencode=arch=compute_120a,code=sm_120a -DFLASHINFER_ENABLE_FP8_E8M0 -DFLASHINFER_ENABLE_FP4_E2M1 -std=c++17 --threads=1 -use_fast_math -DFLASHINFER_ENABLE_F16 -DFLASHINFER_ENABLE_BF16 -DFLASHINFER_ENABLE_FP8_E4M3 -DFLASHINFER_ENABLE_FP8_E5M2 -DNDEBUG -O3 -c /home/m/.cache/flashinfer/0.4.1/120a/generated/batch_decode_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False/batch_decode.cu -o batch_decode_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False/batch_decode.cuda.o 
FAILED: [code=1] batch_decode_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False/batch_decode.cuda.o 
/usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output batch_decode_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False/batch_decode.cuda.o.d -DPy_LIMITED_API=0x03090000 -D_GLIBCXX_USE_CXX11_ABI=1 -isystem /usr/include/python3.12 -isystem /usr/local/cuda/include -isystem /usr/local/cuda/include/cccl -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/tvm_ffi/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/tvm_ffi/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/csrc -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/cutlass/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/cutlass/tools/util/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/spdlog/include --compiler-options=-fPIC --expt-relaxed-constexpr -gencode=arch=compute_120a,code=sm_120a -DFLASHINFER_ENABLE_FP8_E8M0 -DFLASHINFER_ENABLE_FP4_E2M1 -std=c++17 --threads=1 -use_fast_math -DFLASHINFER_ENABLE_F16 -DFLASHINFER_ENABLE_BF16 -DFLASHINFER_ENABLE_FP8_E4M3 -DFLASHINFER_ENABLE_FP8_E5M2 -DNDEBUG -O3 -c /home/m/.cache/flashinfer/0.4.1/120a/generated/batch_decode_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False/batch_decode.cu -o batch_decode_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False/batch_decode.cuda.o 
nvcc fatal   : Unsupported gpu architecture 'compute_120a'
[2/4] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output batch_decode_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False/batch_decode_kernel.cuda.o.d -DPy_LIMITED_API=0x03090000 -D_GLIBCXX_USE_CXX11_ABI=1 -isystem /usr/include/python3.12 -isystem /usr/local/cuda/include -isystem /usr/local/cuda/include/cccl -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/tvm_ffi/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/tvm_ffi/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/csrc -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/cutlass/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/cutlass/tools/util/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/spdlog/include --compiler-options=-fPIC --expt-relaxed-constexpr -gencode=arch=compute_120a,code=sm_120a -DFLASHINFER_ENABLE_FP8_E8M0 -DFLASHINFER_ENABLE_FP4_E2M1 -std=c++17 --threads=1 -use_fast_math -DFLASHINFER_ENABLE_F16 -DFLASHINFER_ENABLE_BF16 -DFLASHINFER_ENABLE_FP8_E4M3 -DFLASHINFER_ENABLE_FP8_E5M2 -DNDEBUG -O3 -c /home/m/.cache/flashinfer/0.4.1/120a/generated/batch_decode_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False/batch_decode_kernel.cu -o batch_decode_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False/batch_decode_kernel.cuda.o 
FAILED: [code=1] batch_decode_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False/batch_decode_kernel.cuda.o 
/usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output batch_decode_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False/batch_decode_kernel.cuda.o.d -DPy_LIMITED_API=0x03090000 -D_GLIBCXX_USE_CXX11_ABI=1 -isystem /usr/include/python3.12 -isystem /usr/local/cuda/include -isystem /usr/local/cuda/include/cccl -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/tvm_ffi/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/tvm_ffi/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/csrc -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/cutlass/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/cutlass/tools/util/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/spdlog/include --compiler-options=-fPIC --expt-relaxed-constexpr -gencode=arch=compute_120a,code=sm_120a -DFLASHINFER_ENABLE_FP8_E8M0 -DFLASHINFER_ENABLE_FP4_E2M1 -std=c++17 --threads=1 -use_fast_math -DFLASHINFER_ENABLE_F16 -DFLASHINFER_ENABLE_BF16 -DFLASHINFER_ENABLE_FP8_E4M3 -DFLASHINFER_ENABLE_FP8_E5M2 -DNDEBUG -O3 -c /home/m/.cache/flashinfer/0.4.1/120a/generated/batch_decode_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False/batch_decode_kernel.cu -o batch_decode_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False/batch_decode_kernel.cuda.o 
nvcc fatal   : Unsupported gpu architecture 'compute_120a'
[3/4] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output batch_decode_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False/batch_decode_jit_binding.cuda.o.d -DPy_LIMITED_API=0x03090000 -D_GLIBCXX_USE_CXX11_ABI=1 -isystem /usr/include/python3.12 -isystem /usr/local/cuda/include -isystem /usr/local/cuda/include/cccl -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/tvm_ffi/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/tvm_ffi/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/csrc -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/cutlass/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/cutlass/tools/util/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/spdlog/include --compiler-options=-fPIC --expt-relaxed-constexpr -gencode=arch=compute_120a,code=sm_120a -DFLASHINFER_ENABLE_FP8_E8M0 -DFLASHINFER_ENABLE_FP4_E2M1 -std=c++17 --threads=1 -use_fast_math -DFLASHINFER_ENABLE_F16 -DFLASHINFER_ENABLE_BF16 -DFLASHINFER_ENABLE_FP8_E4M3 -DFLASHINFER_ENABLE_FP8_E5M2 -DNDEBUG -O3 -c /home/m/.cache/flashinfer/0.4.1/120a/generated/batch_decode_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False/batch_decode_jit_binding.cu -o batch_decode_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False/batch_decode_jit_binding.cuda.o 
FAILED: [code=1] batch_decode_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False/batch_decode_jit_binding.cuda.o 
/usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output batch_decode_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False/batch_decode_jit_binding.cuda.o.d -DPy_LIMITED_API=0x03090000 -D_GLIBCXX_USE_CXX11_ABI=1 -isystem /usr/include/python3.12 -isystem /usr/local/cuda/include -isystem /usr/local/cuda/include/cccl -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/tvm_ffi/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/tvm_ffi/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/csrc -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/cutlass/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/cutlass/tools/util/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/spdlog/include --compiler-options=-fPIC --expt-relaxed-constexpr -gencode=arch=compute_120a,code=sm_120a -DFLASHINFER_ENABLE_FP8_E8M0 -DFLASHINFER_ENABLE_FP4_E2M1 -std=c++17 --threads=1 -use_fast_math -DFLASHINFER_ENABLE_F16 -DFLASHINFER_ENABLE_BF16 -DFLASHINFER_ENABLE_FP8_E4M3 -DFLASHINFER_ENABLE_FP8_E5M2 -DNDEBUG -O3 -c /home/m/.cache/flashinfer/0.4.1/120a/generated/batch_decode_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False/batch_decode_jit_binding.cu -o batch_decode_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False/batch_decode_jit_binding.cuda.o 
nvcc fatal   : Unsupported gpu architecture 'compute_120a'
ninja: build stopped: subcommand failed.

Possible solutions:
1. set --mem-fraction-static to a smaller value (e.g., 0.8 or 0.7)
2. set --cuda-graph-max-bs to a smaller value (e.g., 16)
3. disable torch compile by not using --enable-torch-compile
4. disable CUDA graph by --disable-cuda-graph. (Not recommended. Huge performance loss)
Open an issue on GitHub https://github.com/sgl-project/sglang/issues/new/choose 


[2025-10-28 23:08:33] Received sigquit from a child process. It usually means the child failed.
Killed
(venv) m@m-HP-Z440-Workstation:~/Desktop/sglang/sglang$ 
(venv) m@m-HP-Z440-Workstation:~/Desktop/sglang/sglang$ 
(venv) m@m-HP-Z440-Workstation:~/Desktop/sglang/sglang$ 
(venv) m@m-HP-Z440-Workstation:~/Desktop/sglang/sglang$ 
(venv) m@m-HP-Z440-Workstation:~/Desktop/sglang/sglang$ 
(venv) m@m-HP-Z440-Workstation:~/Desktop/sglang/sglang$ 
(venv) m@m-HP-Z440-Workstation:~/Desktop/sglang/sglang$ 
(venv) m@m-HP-Z440-Workstation:~/Desktop/sglang/sglang$ 
(venv) m@m-HP-Z440-Workstation:~/Desktop/sglang/sglang$ 
(venv) m@m-HP-Z440-Workstation:~/Desktop/sglang/sglang$ 
(venv) m@m-HP-Z440-Workstation:~/Desktop/sglang/sglang$ 
(venv) m@m-HP-Z440-Workstation:~/Desktop/sglang/sglang$ 
(venv) m@m-HP-Z440-Workstation:~/Desktop/sglang/sglang$ 
(venv) m@m-HP-Z440-Workstation:~/Desktop/sglang/sglang$ 
(venv) m@m-HP-Z440-Workstation:~/Desktop/sglang/sglang$ 
(venv) m@m-HP-Z440-Workstation:~/Desktop/sglang/sglang$ 
(venv) m@m-HP-Z440-Workstation:~/Desktop/sglang/sglang$ 
(venv) m@m-HP-Z440-Workstation:~/Desktop/sglang/sglang$ 
(venv) m@m-HP-Z440-Workstation:~/Desktop/sglang/sglang$ 
(venv) m@m-HP-Z440-Workstation:~/Desktop/sglang/sglang$ 
(venv) m@m-HP-Z440-Workstation:~/Desktop/sglang/sglang$ 
(venv) m@m-HP-Z440-Workstation:~/Desktop/sglang/sglang$ 
(venv) m@m-HP-Z440-Workstation:~/Desktop/sglang/sglang$ 
(venv) m@m-HP-Z440-Workstation:~/Desktop/sglang/sglang$ 
(venv) m@m-HP-Z440-Workstation:~/Desktop/sglang/sglang$ 
(venv) m@m-HP-Z440-Workstation:~/Desktop/sglang/sglang$ 
(venv) m@m-HP-Z440-Workstation:~/Desktop/sglang/sglang$ 
(venv) m@m-HP-Z440-Workstation:~/Desktop/sglang/sglang$ 
(venv) m@m-HP-Z440-Workstation:~/Desktop/sglang/sglang$ 
(venv) m@m-HP-Z440-Workstation:~/Desktop/sglang/sglang$ python
Python 3.12.3 (main, Aug 14 2025, 17:47:21) [GCC 13.3.0] on linux
Type "help", "copyright", "credits" or "license" for more information.
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> import sglang as sgl

>>> from sglang.srt.parser.reasoning_parser import ReasoningParser
>>> from sglang.utils import print_highlight
>>> 
>>> llm = sgl.Engine(model_path="facebook/opt-125m")
[2025-10-28 23:11:19] WARNING server_args.py:1104: Attention backend not explicitly specified. Use flashinfer backend by default.
[2025-10-28 23:11:19] INFO engine.py:124: server_args=ServerArgs(model_path='facebook/opt-125m', tokenizer_path='facebook/opt-125m', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=False, context_length=None, is_embedding=False, enable_multimodal=None, revision=None, model_impl='auto', host='127.0.0.1', port=30000, grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, mem_fraction_static=0.771, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=2048, max_prefill_tokens=16384, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=1, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='cuda', tp_size=1, pp_size=1, pp_max_micro_batch_size=None, stream_interval=1, stream_output=False, random_seed=14851919, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, log_level='error', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, oltp_traces_endpoint='localhost:4317', api_key=None, served_model_name='facebook/opt-125m', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='triton', max_lora_chunk_size=16, attention_backend='flashinfer', decode_attention_backend=None, prefill_attention_backend=None, sampling_backend='flashinfer', grammar_backend='xgrammar', mm_attention_backend=None, nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm='static', init_expert_location='trivial', enable_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_amx_weight_path=None, kt_amx_method=None, kt_cpuinfer=None, kt_threadpool_count=None, kt_num_gpu_experts=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=8, cuda_graph_bs=[1, 2, 4, 8], disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8)
[2025-10-28 23:11:28] INFO trace.py:48: opentelemetry package is not installed, tracing disabled
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
Loading pt checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
Loading pt checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.70it/s]
Loading pt checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.70it/s]

Capturing batches (bs=8 avail_mem=2.84 GB):   0%|                                                  | 0/4 [00:01<?, ?it/s]
[2025-10-28 23:11:33] Scheduler hit an exception: Traceback (most recent call last):
  File "/home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/jit/cpp_ext.py", line 282, in run_ninja
    subprocess.run(
  File "/usr/lib/python3.12/subprocess.py", line 571, in run
    raise CalledProcessError(retcode, process.args,
subprocess.CalledProcessError: Command '['ninja', '-v', '-C', '/home/m/.cache/flashinfer/0.4.1/120a/cached_ops', '-f', '/home/m/.cache/flashinfer/0.4.1/120a/cached_ops/batch_decode_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_64_head_dim_vo_64_posenc_0_use_swa_False_use_logits_cap_False/build.ninja']' returned non-zero exit status 1.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/m/Desktop/sglang/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py", line 378, in __init__
    self.capture()
  File "/home/m/Desktop/sglang/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py", line 497, in capture
    ) = self.capture_one_batch_size(bs, forward)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/Desktop/sglang/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py", line 637, in capture_one_batch_size
    self.model_runner.attn_backend.init_forward_metadata_capture_cuda_graph(
  File "/home/m/Desktop/sglang/sglang/python/sglang/srt/layers/attention/flashinfer_backend.py", line 559, in init_forward_metadata_capture_cuda_graph
    self.indices_updater_decode.update(
  File "/home/m/Desktop/sglang/sglang/python/sglang/srt/layers/attention/flashinfer_backend.py", line 902, in update_single_wrapper
    self.call_begin_forward(
  File "/home/m/Desktop/sglang/sglang/python/sglang/srt/layers/attention/flashinfer_backend.py", line 1083, in call_begin_forward
    wrapper.begin_forward(
  File "/home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/decode.py", line 1068, in plan
    self._cached_module = get_batch_decode_module(
                          ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/decode.py", line 211, in get_batch_decode_module
    mod = gen_batch_decode_module(*args).build_and_load()
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/jit/core.py", line 277, in build_and_load
    self.build(verbose, need_lock=False)
  File "/home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/jit/core.py", line 263, in build
    run_ninja(jit_env.FLASHINFER_JIT_DIR, self.ninja_path, verbose)
  File "/home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/jit/cpp_ext.py", line 294, in run_ninja
    raise RuntimeError(msg) from e
RuntimeError: Ninja build failed. Ninja output:
ninja: Entering directory `/home/m/.cache/flashinfer/0.4.1/120a/cached_ops'
[1/4] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output batch_decode_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_64_head_dim_vo_64_posenc_0_use_swa_False_use_logits_cap_False/batch_decode_kernel.cuda.o.d -DPy_LIMITED_API=0x03090000 -D_GLIBCXX_USE_CXX11_ABI=1 -isystem /usr/include/python3.12 -isystem /usr/local/cuda/include -isystem /usr/local/cuda/include/cccl -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/tvm_ffi/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/tvm_ffi/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/csrc -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/cutlass/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/cutlass/tools/util/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/spdlog/include --compiler-options=-fPIC --expt-relaxed-constexpr -gencode=arch=compute_120a,code=sm_120a -DFLASHINFER_ENABLE_FP8_E8M0 -DFLASHINFER_ENABLE_FP4_E2M1 -std=c++17 --threads=1 -use_fast_math -DFLASHINFER_ENABLE_F16 -DFLASHINFER_ENABLE_BF16 -DFLASHINFER_ENABLE_FP8_E4M3 -DFLASHINFER_ENABLE_FP8_E5M2 -DNDEBUG -O3 -c /home/m/.cache/flashinfer/0.4.1/120a/generated/batch_decode_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_64_head_dim_vo_64_posenc_0_use_swa_False_use_logits_cap_False/batch_decode_kernel.cu -o batch_decode_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_64_head_dim_vo_64_posenc_0_use_swa_False_use_logits_cap_False/batch_decode_kernel.cuda.o 
FAILED: [code=1] batch_decode_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_64_head_dim_vo_64_posenc_0_use_swa_False_use_logits_cap_False/batch_decode_kernel.cuda.o 
/usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output batch_decode_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_64_head_dim_vo_64_posenc_0_use_swa_False_use_logits_cap_False/batch_decode_kernel.cuda.o.d -DPy_LIMITED_API=0x03090000 -D_GLIBCXX_USE_CXX11_ABI=1 -isystem /usr/include/python3.12 -isystem /usr/local/cuda/include -isystem /usr/local/cuda/include/cccl -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/tvm_ffi/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/tvm_ffi/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/csrc -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/cutlass/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/cutlass/tools/util/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/spdlog/include --compiler-options=-fPIC --expt-relaxed-constexpr -gencode=arch=compute_120a,code=sm_120a -DFLASHINFER_ENABLE_FP8_E8M0 -DFLASHINFER_ENABLE_FP4_E2M1 -std=c++17 --threads=1 -use_fast_math -DFLASHINFER_ENABLE_F16 -DFLASHINFER_ENABLE_BF16 -DFLASHINFER_ENABLE_FP8_E4M3 -DFLASHINFER_ENABLE_FP8_E5M2 -DNDEBUG -O3 -c /home/m/.cache/flashinfer/0.4.1/120a/generated/batch_decode_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_64_head_dim_vo_64_posenc_0_use_swa_False_use_logits_cap_False/batch_decode_kernel.cu -o batch_decode_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_64_head_dim_vo_64_posenc_0_use_swa_False_use_logits_cap_False/batch_decode_kernel.cuda.o 
nvcc fatal   : Unsupported gpu architecture 'compute_120a'
[2/4] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output batch_decode_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_64_head_dim_vo_64_posenc_0_use_swa_False_use_logits_cap_False/batch_decode_jit_binding.cuda.o.d -DPy_LIMITED_API=0x03090000 -D_GLIBCXX_USE_CXX11_ABI=1 -isystem /usr/include/python3.12 -isystem /usr/local/cuda/include -isystem /usr/local/cuda/include/cccl -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/tvm_ffi/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/tvm_ffi/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/csrc -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/cutlass/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/cutlass/tools/util/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/spdlog/include --compiler-options=-fPIC --expt-relaxed-constexpr -gencode=arch=compute_120a,code=sm_120a -DFLASHINFER_ENABLE_FP8_E8M0 -DFLASHINFER_ENABLE_FP4_E2M1 -std=c++17 --threads=1 -use_fast_math -DFLASHINFER_ENABLE_F16 -DFLASHINFER_ENABLE_BF16 -DFLASHINFER_ENABLE_FP8_E4M3 -DFLASHINFER_ENABLE_FP8_E5M2 -DNDEBUG -O3 -c /home/m/.cache/flashinfer/0.4.1/120a/generated/batch_decode_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_64_head_dim_vo_64_posenc_0_use_swa_False_use_logits_cap_False/batch_decode_jit_binding.cu -o batch_decode_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_64_head_dim_vo_64_posenc_0_use_swa_False_use_logits_cap_False/batch_decode_jit_binding.cuda.o 
FAILED: [code=1] batch_decode_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_64_head_dim_vo_64_posenc_0_use_swa_False_use_logits_cap_False/batch_decode_jit_binding.cuda.o 
/usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output batch_decode_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_64_head_dim_vo_64_posenc_0_use_swa_False_use_logits_cap_False/batch_decode_jit_binding.cuda.o.d -DPy_LIMITED_API=0x03090000 -D_GLIBCXX_USE_CXX11_ABI=1 -isystem /usr/include/python3.12 -isystem /usr/local/cuda/include -isystem /usr/local/cuda/include/cccl -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/tvm_ffi/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/tvm_ffi/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/csrc -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/cutlass/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/cutlass/tools/util/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/spdlog/include --compiler-options=-fPIC --expt-relaxed-constexpr -gencode=arch=compute_120a,code=sm_120a -DFLASHINFER_ENABLE_FP8_E8M0 -DFLASHINFER_ENABLE_FP4_E2M1 -std=c++17 --threads=1 -use_fast_math -DFLASHINFER_ENABLE_F16 -DFLASHINFER_ENABLE_BF16 -DFLASHINFER_ENABLE_FP8_E4M3 -DFLASHINFER_ENABLE_FP8_E5M2 -DNDEBUG -O3 -c /home/m/.cache/flashinfer/0.4.1/120a/generated/batch_decode_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_64_head_dim_vo_64_posenc_0_use_swa_False_use_logits_cap_False/batch_decode_jit_binding.cu -o batch_decode_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_64_head_dim_vo_64_posenc_0_use_swa_False_use_logits_cap_False/batch_decode_jit_binding.cuda.o 
nvcc fatal   : Unsupported gpu architecture 'compute_120a'
[3/4] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output batch_decode_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_64_head_dim_vo_64_posenc_0_use_swa_False_use_logits_cap_False/batch_decode.cuda.o.d -DPy_LIMITED_API=0x03090000 -D_GLIBCXX_USE_CXX11_ABI=1 -isystem /usr/include/python3.12 -isystem /usr/local/cuda/include -isystem /usr/local/cuda/include/cccl -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/tvm_ffi/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/tvm_ffi/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/csrc -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/cutlass/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/cutlass/tools/util/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/spdlog/include --compiler-options=-fPIC --expt-relaxed-constexpr -gencode=arch=compute_120a,code=sm_120a -DFLASHINFER_ENABLE_FP8_E8M0 -DFLASHINFER_ENABLE_FP4_E2M1 -std=c++17 --threads=1 -use_fast_math -DFLASHINFER_ENABLE_F16 -DFLASHINFER_ENABLE_BF16 -DFLASHINFER_ENABLE_FP8_E4M3 -DFLASHINFER_ENABLE_FP8_E5M2 -DNDEBUG -O3 -c /home/m/.cache/flashinfer/0.4.1/120a/generated/batch_decode_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_64_head_dim_vo_64_posenc_0_use_swa_False_use_logits_cap_False/batch_decode.cu -o batch_decode_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_64_head_dim_vo_64_posenc_0_use_swa_False_use_logits_cap_False/batch_decode.cuda.o 
FAILED: [code=1] batch_decode_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_64_head_dim_vo_64_posenc_0_use_swa_False_use_logits_cap_False/batch_decode.cuda.o 
/usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output batch_decode_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_64_head_dim_vo_64_posenc_0_use_swa_False_use_logits_cap_False/batch_decode.cuda.o.d -DPy_LIMITED_API=0x03090000 -D_GLIBCXX_USE_CXX11_ABI=1 -isystem /usr/include/python3.12 -isystem /usr/local/cuda/include -isystem /usr/local/cuda/include/cccl -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/tvm_ffi/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/tvm_ffi/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/csrc -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/cutlass/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/cutlass/tools/util/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/spdlog/include --compiler-options=-fPIC --expt-relaxed-constexpr -gencode=arch=compute_120a,code=sm_120a -DFLASHINFER_ENABLE_FP8_E8M0 -DFLASHINFER_ENABLE_FP4_E2M1 -std=c++17 --threads=1 -use_fast_math -DFLASHINFER_ENABLE_F16 -DFLASHINFER_ENABLE_BF16 -DFLASHINFER_ENABLE_FP8_E4M3 -DFLASHINFER_ENABLE_FP8_E5M2 -DNDEBUG -O3 -c /home/m/.cache/flashinfer/0.4.1/120a/generated/batch_decode_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_64_head_dim_vo_64_posenc_0_use_swa_False_use_logits_cap_False/batch_decode.cu -o batch_decode_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_64_head_dim_vo_64_posenc_0_use_swa_False_use_logits_cap_False/batch_decode.cuda.o 
nvcc fatal   : Unsupported gpu architecture 'compute_120a'
ninja: build stopped: subcommand failed.


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/m/Desktop/sglang/sglang/python/sglang/srt/managers/scheduler.py", line 2753, in run_scheduler_process
    scheduler = Scheduler(
                ^^^^^^^^^^
  File "/home/m/Desktop/sglang/sglang/python/sglang/srt/managers/scheduler.py", line 311, in __init__
    self.tp_worker = TpModelWorker(
                     ^^^^^^^^^^^^^^
  File "/home/m/Desktop/sglang/sglang/python/sglang/srt/managers/tp_worker.py", line 235, in __init__
    self._model_runner = ModelRunner(
                         ^^^^^^^^^^^^
  File "/home/m/Desktop/sglang/sglang/python/sglang/srt/model_executor/model_runner.py", line 312, in __init__
    self.initialize(min_per_gpu_memory)
  File "/home/m/Desktop/sglang/sglang/python/sglang/srt/model_executor/model_runner.py", line 465, in initialize
    self.init_device_graphs()
  File "/home/m/Desktop/sglang/sglang/python/sglang/srt/model_executor/model_runner.py", line 1916, in init_device_graphs
    self.graph_runner = graph_runners[self.device](self)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/Desktop/sglang/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py", line 380, in __init__
    raise Exception(
Exception: Capture cuda graph failed: Ninja build failed. Ninja output:
ninja: Entering directory `/home/m/.cache/flashinfer/0.4.1/120a/cached_ops'
[1/4] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output batch_decode_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_64_head_dim_vo_64_posenc_0_use_swa_False_use_logits_cap_False/batch_decode_kernel.cuda.o.d -DPy_LIMITED_API=0x03090000 -D_GLIBCXX_USE_CXX11_ABI=1 -isystem /usr/include/python3.12 -isystem /usr/local/cuda/include -isystem /usr/local/cuda/include/cccl -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/tvm_ffi/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/tvm_ffi/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/csrc -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/cutlass/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/cutlass/tools/util/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/spdlog/include --compiler-options=-fPIC --expt-relaxed-constexpr -gencode=arch=compute_120a,code=sm_120a -DFLASHINFER_ENABLE_FP8_E8M0 -DFLASHINFER_ENABLE_FP4_E2M1 -std=c++17 --threads=1 -use_fast_math -DFLASHINFER_ENABLE_F16 -DFLASHINFER_ENABLE_BF16 -DFLASHINFER_ENABLE_FP8_E4M3 -DFLASHINFER_ENABLE_FP8_E5M2 -DNDEBUG -O3 -c /home/m/.cache/flashinfer/0.4.1/120a/generated/batch_decode_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_64_head_dim_vo_64_posenc_0_use_swa_False_use_logits_cap_False/batch_decode_kernel.cu -o batch_decode_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_64_head_dim_vo_64_posenc_0_use_swa_False_use_logits_cap_False/batch_decode_kernel.cuda.o 
FAILED: [code=1] batch_decode_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_64_head_dim_vo_64_posenc_0_use_swa_False_use_logits_cap_False/batch_decode_kernel.cuda.o 
/usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output batch_decode_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_64_head_dim_vo_64_posenc_0_use_swa_False_use_logits_cap_False/batch_decode_kernel.cuda.o.d -DPy_LIMITED_API=0x03090000 -D_GLIBCXX_USE_CXX11_ABI=1 -isystem /usr/include/python3.12 -isystem /usr/local/cuda/include -isystem /usr/local/cuda/include/cccl -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/tvm_ffi/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/tvm_ffi/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/csrc -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/cutlass/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/cutlass/tools/util/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/spdlog/include --compiler-options=-fPIC --expt-relaxed-constexpr -gencode=arch=compute_120a,code=sm_120a -DFLASHINFER_ENABLE_FP8_E8M0 -DFLASHINFER_ENABLE_FP4_E2M1 -std=c++17 --threads=1 -use_fast_math -DFLASHINFER_ENABLE_F16 -DFLASHINFER_ENABLE_BF16 -DFLASHINFER_ENABLE_FP8_E4M3 -DFLASHINFER_ENABLE_FP8_E5M2 -DNDEBUG -O3 -c /home/m/.cache/flashinfer/0.4.1/120a/generated/batch_decode_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_64_head_dim_vo_64_posenc_0_use_swa_False_use_logits_cap_False/batch_decode_kernel.cu -o batch_decode_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_64_head_dim_vo_64_posenc_0_use_swa_False_use_logits_cap_False/batch_decode_kernel.cuda.o 
nvcc fatal   : Unsupported gpu architecture 'compute_120a'
[2/4] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output batch_decode_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_64_head_dim_vo_64_posenc_0_use_swa_False_use_logits_cap_False/batch_decode_jit_binding.cuda.o.d -DPy_LIMITED_API=0x03090000 -D_GLIBCXX_USE_CXX11_ABI=1 -isystem /usr/include/python3.12 -isystem /usr/local/cuda/include -isystem /usr/local/cuda/include/cccl -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/tvm_ffi/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/tvm_ffi/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/csrc -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/cutlass/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/cutlass/tools/util/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/spdlog/include --compiler-options=-fPIC --expt-relaxed-constexpr -gencode=arch=compute_120a,code=sm_120a -DFLASHINFER_ENABLE_FP8_E8M0 -DFLASHINFER_ENABLE_FP4_E2M1 -std=c++17 --threads=1 -use_fast_math -DFLASHINFER_ENABLE_F16 -DFLASHINFER_ENABLE_BF16 -DFLASHINFER_ENABLE_FP8_E4M3 -DFLASHINFER_ENABLE_FP8_E5M2 -DNDEBUG -O3 -c /home/m/.cache/flashinfer/0.4.1/120a/generated/batch_decode_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_64_head_dim_vo_64_posenc_0_use_swa_False_use_logits_cap_False/batch_decode_jit_binding.cu -o batch_decode_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_64_head_dim_vo_64_posenc_0_use_swa_False_use_logits_cap_False/batch_decode_jit_binding.cuda.o 
FAILED: [code=1] batch_decode_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_64_head_dim_vo_64_posenc_0_use_swa_False_use_logits_cap_False/batch_decode_jit_binding.cuda.o 
/usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output batch_decode_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_64_head_dim_vo_64_posenc_0_use_swa_False_use_logits_cap_False/batch_decode_jit_binding.cuda.o.d -DPy_LIMITED_API=0x03090000 -D_GLIBCXX_USE_CXX11_ABI=1 -isystem /usr/include/python3.12 -isystem /usr/local/cuda/include -isystem /usr/local/cuda/include/cccl -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/tvm_ffi/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/tvm_ffi/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/csrc -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/cutlass/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/cutlass/tools/util/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/spdlog/include --compiler-options=-fPIC --expt-relaxed-constexpr -gencode=arch=compute_120a,code=sm_120a -DFLASHINFER_ENABLE_FP8_E8M0 -DFLASHINFER_ENABLE_FP4_E2M1 -std=c++17 --threads=1 -use_fast_math -DFLASHINFER_ENABLE_F16 -DFLASHINFER_ENABLE_BF16 -DFLASHINFER_ENABLE_FP8_E4M3 -DFLASHINFER_ENABLE_FP8_E5M2 -DNDEBUG -O3 -c /home/m/.cache/flashinfer/0.4.1/120a/generated/batch_decode_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_64_head_dim_vo_64_posenc_0_use_swa_False_use_logits_cap_False/batch_decode_jit_binding.cu -o batch_decode_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_64_head_dim_vo_64_posenc_0_use_swa_False_use_logits_cap_False/batch_decode_jit_binding.cuda.o 
nvcc fatal   : Unsupported gpu architecture 'compute_120a'
[3/4] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output batch_decode_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_64_head_dim_vo_64_posenc_0_use_swa_False_use_logits_cap_False/batch_decode.cuda.o.d -DPy_LIMITED_API=0x03090000 -D_GLIBCXX_USE_CXX11_ABI=1 -isystem /usr/include/python3.12 -isystem /usr/local/cuda/include -isystem /usr/local/cuda/include/cccl -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/tvm_ffi/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/tvm_ffi/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/csrc -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/cutlass/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/cutlass/tools/util/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/spdlog/include --compiler-options=-fPIC --expt-relaxed-constexpr -gencode=arch=compute_120a,code=sm_120a -DFLASHINFER_ENABLE_FP8_E8M0 -DFLASHINFER_ENABLE_FP4_E2M1 -std=c++17 --threads=1 -use_fast_math -DFLASHINFER_ENABLE_F16 -DFLASHINFER_ENABLE_BF16 -DFLASHINFER_ENABLE_FP8_E4M3 -DFLASHINFER_ENABLE_FP8_E5M2 -DNDEBUG -O3 -c /home/m/.cache/flashinfer/0.4.1/120a/generated/batch_decode_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_64_head_dim_vo_64_posenc_0_use_swa_False_use_logits_cap_False/batch_decode.cu -o batch_decode_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_64_head_dim_vo_64_posenc_0_use_swa_False_use_logits_cap_False/batch_decode.cuda.o 
FAILED: [code=1] batch_decode_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_64_head_dim_vo_64_posenc_0_use_swa_False_use_logits_cap_False/batch_decode.cuda.o 
/usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output batch_decode_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_64_head_dim_vo_64_posenc_0_use_swa_False_use_logits_cap_False/batch_decode.cuda.o.d -DPy_LIMITED_API=0x03090000 -D_GLIBCXX_USE_CXX11_ABI=1 -isystem /usr/include/python3.12 -isystem /usr/local/cuda/include -isystem /usr/local/cuda/include/cccl -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/tvm_ffi/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/tvm_ffi/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/csrc -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/cutlass/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/cutlass/tools/util/include -isystem /home/m/Desktop/sglang/venv/lib/python3.12/site-packages/flashinfer/data/spdlog/include --compiler-options=-fPIC --expt-relaxed-constexpr -gencode=arch=compute_120a,code=sm_120a -DFLASHINFER_ENABLE_FP8_E8M0 -DFLASHINFER_ENABLE_FP4_E2M1 -std=c++17 --threads=1 -use_fast_math -DFLASHINFER_ENABLE_F16 -DFLASHINFER_ENABLE_BF16 -DFLASHINFER_ENABLE_FP8_E4M3 -DFLASHINFER_ENABLE_FP8_E5M2 -DNDEBUG -O3 -c /home/m/.cache/flashinfer/0.4.1/120a/generated/batch_decode_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_64_head_dim_vo_64_posenc_0_use_swa_False_use_logits_cap_False/batch_decode.cu -o batch_decode_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_64_head_dim_vo_64_posenc_0_use_swa_False_use_logits_cap_False/batch_decode.cuda.o 
nvcc fatal   : Unsupported gpu architecture 'compute_120a'
ninja: build stopped: subcommand failed.

Possible solutions:
1. set --mem-fraction-static to a smaller value (e.g., 0.8 or 0.7)
2. set --cuda-graph-max-bs to a smaller value (e.g., 16)
3. disable torch compile by not using --enable-torch-compile
4. disable CUDA graph by --disable-cuda-graph. (Not recommended. Huge performance loss)
Open an issue on GitHub https://github.com/sgl-project/sglang/issues/new/choose 


[2025-10-28 23:11:33] Received sigquit from a child process. It usually means the child failed.
Killed
(venv) m@m-HP-Z440-Workstation:~/Desktop/sglang/sglang$ tokenizer = AutoTokenizer.from_pretrained("facebook/opt-125m")
bash: syntax error near unexpected token `('
(venv) m@m-HP-Z440-Workstation:~/Desktop/sglang/sglang$ input = tokenizer.apply_chat_template(
bash: syntax error near unexpected token `('
(venv) m@m-HP-Z440-Workstation:~/Desktop/sglang/sglang$     messages,
Command 'messages,' not found, did you mean:
  command 'messages' from deb mailutils (1:3.16-1build1)
Try: sudo apt install <deb name>
(venv) m@m-HP-Z440-Workstation:~/Desktop/sglang/sglang$     tokenize=False,
(venv) m@m-HP-Z440-Workstation:~/Desktop/sglang/sglang$     add_generation_prompt=True,
(venv) m@m-HP-Z440-Workstation:~/Desktop/sglang/sglang$ )
bash: syntax error near unexpected token `)'
(venv) m@m-HP-Z440-Workstation:~/Desktop/sglang/sglang$ sampling_params = {
sampling_params: command not found
(venv) m@m-HP-Z440-Workstation:~/Desktop/sglang/sglang$     "max_new_tokens": 1024,
max_new_tokens:: command not found
(venv) m@m-HP-Z440-Workstation:~/Desktop/sglang/sglang$     "skip_special_tokens": False,
skip_special_tokens:: command not found
(venv) m@m-HP-Z440-Workstation:~/Desktop/sglang/sglang$     "temperature": 0.6,
temperature:: command not found
(venv) m@m-HP-Z440-Workstation:~/Desktop/sglang/sglang$     "top_p": 0.95,
top_p:: command not found
(venv) m@m-HP-Z440-Workstation:~/Desktop/sglang/sglang$ }
bash: syntax error near unexpected token `}'
(venv) m@m-HP-Z440-Workstation:~/Desktop/sglang/sglang$ result = llm.generate(prompt=input, sampling_params=sampling_params)
bash: syntax error near unexpected token `('
(venv) m@m-HP-Z440-Workstation:~/Desktop/sglang/sglang$ 
(venv) m@m-HP-Z440-Workstation:~/Desktop/sglang/sglang$ generated_text = result["text"]  # Assume there is only one prompt
generated_text: command not found
(venv) m@m-HP-Z440-Workstation:~/Desktop/sglang/sglang$ 
(venv) m@m-HP-Z440-Workstation:~/Desktop/sglang/sglang$ print_highlight("==== Original Output ====")
bash: syntax error near unexpected token `"==== Original Output ===="'
(venv) m@m-HP-Z440-Workstation:~/Desktop/sglang/sglang$ print_highlight(generated_text)
bash: syntax error near unexpected token `generated_text'
(venv) m@m-HP-Z440-Workstation:~/Desktop/sglang/sglang$ 
(venv) m@m-HP-Z440-Workstation:~/Desktop/sglang/sglang$ parser = ReasoningParser("deepseek-r1")
bash: syntax error near unexpected token `('
(venv) m@m-HP-Z440-Workstation:~/Desktop/sglang/sglang$ reasoning_text, text = parser.parse_non_stream(generated_text)
bash: syntax error near unexpected token `('
(venv) m@m-HP-Z440-Workstation:~/Desktop/sglang/sglang$ print_highlight("==== Reasoning ====")
bash: syntax error near unexpected token `"==== Reasoning ===="'
(venv) m@m-HP-Z440-Workstation:~/Desktop/sglang/sglang$ print_highlight(reasoning_text)
bash: syntax error near unexpected token `reasoning_text'
(venv) m@m-HP-Z440-Workstation:~/Desktop/sglang/sglang$ print_highlight("==== Text ====")
bash: syntax error near unexpected token `"==== Text ===="'
(venv) m@m-HP-Z440-Workstation:~/Desktop/sglang/sglang$ print_highlight(text)
bash: syntax error near unexpected token `text'
(venv) m@m-HP-Z440-Workstation:~/Desktop/sglang/sglang$ 
(venv) m@m-HP-Z440-Workstation:~/Desktop/sglang/sglang$ 
(venv) m@m-HP-Z440-Workstation:~/Desktop/sglang/sglang$ 
(venv) m@m-HP-Z440-Workstation:~/Desktop/sglang/sglang$ 
(venv) m@m-HP-Z440-Workstation:~/Desktop/sglang/sglang$ 
(venv) m@m-HP-Z440-Workstation:~/Desktop/sglang/sglang$ 
(venv) m@m-HP-Z440-Workstation:~/Desktop/sglang/sglang$ 
(venv) m@m-HP-Z440-Workstation:~/Desktop/sglang/sglang$ 
(venv) m@m-HP-Z440-Workstation:~/Desktop/sglang/sglang$ 
(venv) m@m-HP-Z440-Workstation:~/Desktop/sglang/sglang$ 
(venv) m@m-HP-Z440-Workstation:~/Desktop/sglang/sglang$ 
(venv) m@m-HP-Z440-Workstation:~/Desktop/sglang/sglang$ 
(venv) m@m-HP-Z440-Workstation:~/Desktop/sglang/sglang$ 
(venv) m@m-HP-Z440-Workstation:~/Desktop/sglang/sglang$ 
(venv) m@m-HP-Z440-Workstation:~/Desktop/sglang/sglang$ 
(venv) m@m-HP-Z440-Workstation:~/Desktop/sglang/sglang$ 
(venv) m@m-HP-Z440-Workstation:~/Desktop/sglang/sglang$ 
(venv) m@m-HP-Z440-Workstation:~/Desktop/sglang/sglang$ 
(venv) m@m-HP-Z440-Workstation:~/Desktop/sglang/sglang$ 
(venv) m@m-HP-Z440-Workstation:~/Desktop/sglang/sglang$ 
(venv) m@m-HP-Z440-Workstation:~/Desktop/sglang/sglang$ 
(venv) m@m-HP-Z440-Workstation:~/Desktop/sglang/sglang$ 
(venv) m@m-HP-Z440-Workstation:~/Desktop/sglang/sglang$ 
(venv) m@m-HP-Z440-Workstation:~/Desktop/sglang/sglang$ 
(venv) m@m-HP-Z440-Workstation:~/Desktop/sglang/sglang$ 
(venv) m@m-HP-Z440-Workstation:~/Desktop/sglang/sglang$ 
(venv) m@m-HP-Z440-Workstation:~/Desktop/sglang/sglang$ GL_ENABLE_JIT_DEEPGEMM=0 python -m sglang.launch_server --model /home/m/huggingface/Qwen3-0.6B --tp 1  --host 0.0.0.0 --port  5000 --mem-fraction-static 0.8 --context-length 64  --enable-metrics  --attention-backend triton --chunked-prefill-size 500
[2025-10-28 23:16:17] INFO trace.py:48: opentelemetry package is not installed, tracing disabled
[2025-10-28 23:16:17] server_args=ServerArgs(model_path='/home/m/huggingface/Qwen3-0.6B', tokenizer_path='/home/m/huggingface/Qwen3-0.6B', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=False, context_length=64, is_embedding=False, enable_multimodal=None, revision=None, model_impl='auto', host='0.0.0.0', port=5000, grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, mem_fraction_static=0.8, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=500, max_prefill_tokens=16384, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=1, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='cuda', tp_size=1, pp_size=1, pp_max_micro_batch_size=None, stream_interval=1, stream_output=False, random_seed=5232156, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=True, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, oltp_traces_endpoint='localhost:4317', api_key=None, served_model_name='/home/m/huggingface/Qwen3-0.6B', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='triton', max_lora_chunk_size=16, attention_backend='triton', decode_attention_backend=None, prefill_attention_backend=None, sampling_backend='flashinfer', grammar_backend='xgrammar', mm_attention_backend=None, nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm='static', init_expert_location='trivial', enable_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_amx_weight_path=None, kt_amx_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=8, cuda_graph_bs=[1, 2, 4, 8], disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8)
[2025-10-28 23:16:18] Using default HuggingFace chat template with detected content format: string
[2025-10-28 23:16:25] INFO trace.py:48: opentelemetry package is not installed, tracing disabled
[2025-10-28 23:16:25] INFO trace.py:48: opentelemetry package is not installed, tracing disabled
[2025-10-28 23:16:26] Init torch distributed begin.
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[2025-10-28 23:16:26] Init torch distributed ends. mem usage=0.00 GB
[2025-10-28 23:16:26] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2025-10-28 23:16:27] Load weight begin. avail mem=15.13 GB
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.70it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.70it/s]

[2025-10-28 23:16:27] Load weight end. type=Qwen3ForCausalLM, dtype=torch.bfloat16, avail mem=13.85 GB, mem usage=1.28 GB.
[2025-10-28 23:16:27] Using KV cache dtype: torch.bfloat16
[2025-10-28 23:16:28] KV Cache is allocated. #tokens: 101329, K size: 5.41 GB, V size: 5.41 GB
[2025-10-28 23:16:28] Memory pool end. avail mem=3.02 GB
[2025-10-28 23:16:28] Capture cuda graph begin. This can take up to several minutes. avail mem=3.00 GB
[2025-10-28 23:16:28] Capture cuda graph bs [1, 2, 4, 8]
Capturing batches (bs=1 avail_mem=2.94 GB): 100%|██████████████████████████████████████████| 4/4 [00:03<00:00,  1.25it/s]
[2025-10-28 23:16:31] Capture cuda graph end. Time elapsed: 3.75 s. mem usage=0.06 GB. avail mem=2.94 GB.
[2025-10-28 23:16:32] max_total_num_tokens=101329, chunked_prefill_size=500, max_prefill_tokens=16384, max_running_requests=4096, context_len=64, available_gpu_mem=2.95 GB
[2025-10-28 23:16:32] INFO:     Started server process [11038]
[2025-10-28 23:16:32] INFO:     Waiting for application startup.
[2025-10-28 23:16:32] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[2025-10-28 23:16:32] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[2025-10-28 23:16:32] INFO:     Application startup complete.
[2025-10-28 23:16:32] INFO:     Uvicorn running on http://0.0.0.0:5000 (Press CTRL+C to quit)
[2025-10-28 23:16:33] INFO:     127.0.0.1:60316 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-10-28 23:16:33] Prefill batch, #new-seq: 1, #new-token: 6, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-28 23:16:38] INFO:     127.0.0.1:60324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 23:16:38] The server is fired up and ready to roll!
^CProcess Process-2:
Traceback (most recent call last):
  File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/m/Desktop/sglang/sglang/python/sglang/srt/managers/detokenizer_manager.py", line 325, in run_detokenizer_process
    manager.event_loop()
  File "/home/m/Desktop/sglang/sglang/python/sglang/srt/managers/detokenizer_manager.py", line 116, in event_loop
    recv_obj = self.recv_from_scheduler.recv_pyobj()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/Desktop/sglang/venv/lib/python3.12/site-packages/zmq/sugar/socket.py", line 989, in recv_pyobj
    msg = self.recv(flags)
          ^^^^^^^^^^^^^^^^
  File "zmq/backend/cython/_zmq.py", line 1218, in zmq.backend.cython._zmq.Socket.recv
    def recv(self, flags=0, copy: bint = True, track: bint = False):
    ^^^^^^^^^^^
  File "zmq/backend/cython/_zmq.py", line 1253, in zmq.backend.cython._zmq.Socket.recv
    return _recv_copy(self.handle, flags)
    ^^^^^^^^^^^
  File "zmq/backend/cython/_zmq.py", line 1408, in zmq.backend.cython._zmq._recv_copy
    _check_rc(rc)
    ^^^^^^^^^^^
  File "zmq/backend/cython/_zmq.py", line 179, in zmq.backend.cython._zmq._check_rc
    PyErr_CheckSignals()
    ^^^^^^^^^^^
KeyboardInterrupt
[2025-10-28 23:21:13] INFO:     Shutting down
[2025-10-28 23:21:14] INFO:     Waiting for application shutdown.
[2025-10-28 23:21:14] INFO:     Application shutdown complete.
[2025-10-28 23:21:14] INFO:     Finished server process [11038]
^CException ignored in atexit callback: <bound method finalize._exitfunc of <class 'weakref.finalize'>>
Traceback (most recent call last):
  File "/usr/lib/python3.12/weakref.py", line 666, in _exitfunc
    f()
  File "/usr/lib/python3.12/weakref.py", line 590, in __call__
    return info.func(*info.args, **(info.kwargs or {}))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/Desktop/sglang/venv/lib/python3.12/site-packages/torch/library.py", line 482, in _del_library
    m.reset()
KeyboardInterrupt: 
(venv) m@m-HP-Z440-Workstation:~/Desktop/sglang/sglang$ 

