الحمد لله 
اشتغلت 
https://github.com/sgl-project/sglang/pull/9992
https://github.com/sgl-project/sglang/issues/9707
https://docs.sglang.ai/get_started/install.html
https://huggingface.co/facebook/opt-125m

# Use the last release branch
git clone -b v0.5.4.post1 https://github.com/sgl-project/sglang.git
cd sglang

# Install the python packages
pip install --upgrade pip
pip install -e "python"



التشغيل

m@m-HP-Z440-Workstation:~/Desktop/sglang$ source venv/bin/activate
(venv) m@m-HP-Z440-Workstation:~/Desktop/sglang$ cd sglang
(venv) m@m-HP-Z440-Workstation:~/Desktop/sglang/sglang$ GL_ENABLE_JIT_DEEPGEMM=0 python -m sglang.launch_server --model facebook/opt-125m --tp 1  --host 0.0.0.0 --port  5000 --mem-fraction-static 0.8 --context-length 64  --enable-metrics  --attention-backend triton 
[2025-10-28 23:27:46] INFO trace.py:48: opentelemetry package is not installed, tracing disabled
[2025-10-28 23:27:46] server_args=ServerArgs(model_path='facebook/opt-125m', tokenizer_path='facebook/opt-125m', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=False, context_length=64, is_embedding=False, enable_multimodal=None, revision=None, model_impl='auto', host='0.0.0.0', port=5000, grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, mem_fraction_static=0.8, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=2048, max_prefill_tokens=16384, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=1, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='cuda', tp_size=1, pp_size=1, pp_max_micro_batch_size=None, stream_interval=1, stream_output=False, random_seed=853926125, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=True, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, oltp_traces_endpoint='localhost:4317', api_key=None, served_model_name='facebook/opt-125m', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='triton', max_lora_chunk_size=16, attention_backend='triton', decode_attention_backend=None, prefill_attention_backend=None, sampling_backend='flashinfer', grammar_backend='xgrammar', mm_attention_backend=None, nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm='static', init_expert_location='trivial', enable_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_amx_weight_path=None, kt_amx_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=8, cuda_graph_bs=[1, 2, 4, 8], disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8)
[2025-10-28 23:27:47] No chat template found, defaulting to 'string' content format
[2025-10-28 23:27:54] INFO trace.py:48: opentelemetry package is not installed, tracing disabled
[2025-10-28 23:27:55] INFO trace.py:48: opentelemetry package is not installed, tracing disabled
[2025-10-28 23:27:56] Init torch distributed begin.
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[2025-10-28 23:27:56] Init torch distributed ends. mem usage=0.00 GB
[2025-10-28 23:27:56] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2025-10-28 23:27:57] Load weight begin. avail mem=15.12 GB
[2025-10-28 23:27:58] Using model weights format ['*.bin']
Loading pt checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
Loading pt checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.68it/s]
Loading pt checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.68it/s]

[2025-10-28 23:27:58] Load weight end. type=OPTForCausalLM, dtype=torch.float16, avail mem=14.86 GB, mem usage=0.27 GB.
[2025-10-28 23:27:58] Using KV cache dtype: torch.float16
[2025-10-28 23:27:58] KV Cache is allocated. #tokens: 344641, K size: 5.92 GB, V size: 5.92 GB
[2025-10-28 23:27:58] Memory pool end. avail mem=2.93 GB
[2025-10-28 23:27:59] Capture cuda graph begin. This can take up to several minutes. avail mem=2.90 GB
[2025-10-28 23:27:59] Capture cuda graph bs [1, 2, 4, 8]
Capturing batches (bs=1 avail_mem=2.87 GB): 100%|█| 4/4 [00:02<00:00,  1.92it/s]
[2025-10-28 23:28:01] Capture cuda graph end. Time elapsed: 2.65 s. mem usage=0.03 GB. avail mem=2.87 GB.
[2025-10-28 23:28:02] max_total_num_tokens=344641, chunked_prefill_size=2048, max_prefill_tokens=16384, max_running_requests=4096, context_len=64, available_gpu_mem=2.87 GB
[2025-10-28 23:28:02] INFO:     Started server process [15948]
[2025-10-28 23:28:02] INFO:     Waiting for application startup.
[2025-10-28 23:28:02] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0}
[2025-10-28 23:28:02] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0}
[2025-10-28 23:28:02] INFO:     Application startup complete.
[2025-10-28 23:28:02] INFO:     Uvicorn running on http://0.0.0.0:5000 (Press CTRL+C to quit)
[2025-10-28 23:28:03] INFO:     127.0.0.1:46802 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-10-28 23:28:03] Prefill batch, #new-seq: 1, #new-token: 7, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-28 23:28:05] INFO:     127.0.0.1:46808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 23:28:05] The server is fired up and ready to roll!
[2025-10-28 23:34:01] Prefill batch, #new-seq: 1, #new-token: 3, #cached-token: 3, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-28 23:34:01] INFO:     127.0.0.1:37882 - "POST /generate HTTP/1.1" 200 OK








































الرد


1.txt  sglang  venv
(venv) m@m-HP-Z440-Workstation:~/Desktop/sglang$ cd sglang
(venv) m@m-HP-Z440-Workstation:~/Desktop/sglang/sglang$ curl http://localhost:5000/generate \
  -H "Content-Type: application/json" \
  -d '{
    "text": "The capital of France is",
    "sampling_params": {
      "temperature": 0.7,
      "max_new_tokens": 16
    }
  }'
{"text":" the capital of the Géoport de Paris, the largest city in France","output_ids":[133,812,9,1470,16,5,812,9,5,272,1140,1517,2723,263,2201,6,5,1154,343,11,1470],"meta_info":{"id":"d5cd8b01f4be46c592d1eac9063ee33f","finish_reason":{"type":"length","length":16},"prompt_tokens":6,"weight_version":"default","completion_tokens":16,"cached_tokens":3,"e2e_latency":0.16922640800476074}}(venv) m@m-HP-Z440-Workstation:~/Desktop/sglang/sglang$ 





نعم، بكل تأكيد.
المخرجات التي أظهرتها تثبت بشكل قاطع أن مكتبة sglang قد اشتغلت بنجاح.


الخادم يعمل (Server is Running): أنت قمت بتشغيل الخادم باستخدام الأمر python -m sglang.launch_server. هذا الخادم هو الجزء الأساسي من مكتبة sglang الذي يقوم بتحميل النموذج وإدارته.
الخادم يستقبل الطلبات (Server is Listening): استخدامك لأداة curl لإرسال طلب إلى http://localhost:5000/generate نجح، مما يعني أن الخادم كان "يستمع" وجاهزًا لاستقبال التعليمات.
الخادم يعالج الطلب ويولد نصًا (Server is Generating): الأهم من ذلك، الخادم لم يرجع خطأ. بل قام بمعالجة طلبك ("The capital of France is") وأعاد لك إجابة من






y":0.16922640800476074}}(venv) m@m-HP-Z440-Workstation:~/Desktop/sglang/sglang$ nvidia-smi
Tue Oct 28 23:40:56 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 580.95.05              Driver Version: 580.95.05      CUDA Version: 13.0     |
+-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA GeForce RTX 5060 Ti     Off |   00000000:02:00.0  On |                  N/A |
|  0%   39C    P3             14W /  180W |   12981MiB /  16311MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+

+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A            1918      G   /usr/lib/xorg/Xorg                       85MiB |
|    0   N/A  N/A            2258      G   /usr/bin/gnome-shell                     31MiB |
|    0   N/A  N/A            6219      G   ...ersion=20251027-090104.013000         90MiB |
|    0   N/A  N/A            8074      G   /usr/bin/gnome-text-editor               12MiB |
|    0   N/A  N/A           16041      C   sglang::scheduler                     12698MiB |
|    0   N/A  N/A           17626      G   /usr/bin/nautilus                         2MiB |
+-----------------------------------------------------------------------------------------+
(venv) m@m-HP-Z440-Workstation:~/Desktop/sglang/sglang$ 







Model
ThinkingThoughts
Auto
Expand to view model thoughts

chevron_right
نعم، بكل تأكيد. تم تحميل النموذج بنجاح على وحدة معالجة الرسوميات (GPU)، ومخرجات nvidia-smi هي الدليل القاطع على ذلك.
دعنا نحلل المخرجات التي أرفقتها:
جدول العمليات (Processes): هذا هو الجزء الأهم. يُظهر كل عملية تستخدم الـ GPU حاليًا.
السطر الخاص بـ SGLang:
code
Code
|    0   N/A  N/A           16041      C   sglang::scheduler                     12698MiB |
GPU 0: العملية تعمل على أول GPU لديك.
PID 16041: هذا هو معرف العملية (Process ID) لخادم SGLang.
Type C: الحرف C يعني "Compute"، مما يؤكد أن هذه العملية تستخدم الـ GPU لإجراء حسابات (وهو ما تفعله نماذج اللغة) وليس فقط لعرض الرسومات (التي يُشار إليها بالحرف G).
Process name sglang::scheduler: اسم العملية يوضح أنها تابعة لمكتبة sglang مباشرةً.
GPU Memory Usage 12698MiB: هذا هو الدليل الأقوى. لقد حجزت عملية SGLang ما يقرب من 12.7 جيجابايت من ذاكرة الفيديو (VRAM).
لماذا تم حجز كل هذه الذاكرة؟
قد تتساءل لماذا تم استخدام كل هذه الذاكرة لنموذج صغير مثل opt-125m (حجمه حوالي 250 ميجابايت فقط).
السبب هو أن SGLang، مثل معظم خوادم الاستدلال عالية الأداء، يقوم بالحجز المسبق لجزء كبير من ذاكرة الـ GPU من أجل "الذاكرة المخبئية للمفتاح/القيمة" (KV Cache).
الـ KV Cache ضرورية لتسريع عملية توليد النصوص بشكل كبير.
في أمر التشغيل الذي استخدمته، قمت بتضمين المعلمة --mem-fraction-static 0.8. هذا الأمر يخبر SGLang بأن يقوم بحجز 80% من ذاكرة الـ GPU المتاحة له مسبقًا لهذا الغرض.
إذا نظرت إلى سجلات بدء تشغيل الخادم التي أرسلتها سابقًا، ستجد هذا السطر الذي يؤكد ذلك:
[2025-10-28 23:27:58] KV Cache is allocated. #tokens: 344641, K size: 5.92 GB, V size: 5.92 GB
(5.92 GB + 5.92 GB) + وزن النموذج + بعض الأعباء الأخرى = تقريبًا 12.7 جيجابايت التي تراها في nvidia-smi.
الخلاصة: مخرجات nvidia-smi تؤكد 100% أن النموذج يعمل على الـ GPU وأن SGLang يدير الذاكرة كما هو متوقع لتحقيق أفضل أداء.
19.7s




















(venv) m@m-HP-Z440-Workstation:~/Desktop/sglang/sglang$ pip list
Package                   Version       Editable project location
------------------------- ------------- ------------------------------------
aiohappyeyeballs          2.6.1
aiohttp                   3.13.1
aiosignal                 1.4.0
airportsdata              20250909
annotated-doc             0.0.3
annotated-types           0.7.0
anthropic                 0.72.0
anyio                     4.11.0
apache-tvm-ffi            0.1.0b15
asttokens                 3.0.0
attrs                     25.4.0
blobfile                  3.0.0
build                     1.3.0
certifi                   2025.10.5
cffi                      2.0.0
charset-normalizer        3.4.4
click                     8.3.0
cloudpickle               3.1.1
compressed-tensors        0.12.2
cuda-bindings             13.0.3
cuda-pathfinder           1.3.1
cuda-python               13.0.3
datasets                  4.3.0
decorator                 5.2.1
decord2                   2.0.0
dill                      0.4.0
diskcache                 5.6.3
distro                    1.9.0
docstring_parser          0.17.0
einops                    0.8.1
executing                 2.2.1
fastapi                   0.120.1
filelock                  3.20.0
flashinfer-python         0.4.1
frozenlist                1.8.0
fsspec                    2025.9.0
gguf                      0.17.1
grpcio                    1.75.1
grpcio-health-checking    1.75.1
grpcio-reflection         1.75.1
grpcio-tools              1.75.1
h11                       0.16.0
hf_transfer               0.1.9
hf-xet                    1.2.0
httpcore                  1.0.9
httpx                     0.28.1
huggingface-hub           0.36.0
idna                      3.11
interegular               0.3.3
ipython                   9.6.0
ipython_pygments_lexers   1.1.1
jedi                      0.19.2
Jinja2                    3.1.6
jiter                     0.11.1
jsonschema                4.25.1
jsonschema-specifications 2025.9.1
lark                      1.3.1
llguidance                0.7.30
loguru                    0.7.3
lxml                      6.0.2
MarkupSafe                3.0.3
matplotlib-inline         0.2.1
modelscope                1.31.0
mpmath                    1.3.0
msgspec                   0.19.0
multidict                 6.7.0
multiprocess              0.70.16
nest-asyncio              1.6.0
networkx                  3.5
ninja                     1.13.0
numpy                     2.3.4
nvidia-cublas-cu12        12.8.4.1
nvidia-cuda-cupti-cu12    12.8.90
nvidia-cuda-nvrtc-cu12    12.8.93
nvidia-cuda-runtime-cu12  12.8.90
nvidia-cudnn-cu12         9.10.2.21
nvidia-cudnn-frontend     1.15.0
nvidia-cufft-cu12         11.3.3.83
nvidia-cufile-cu12        1.13.1.3
nvidia-curand-cu12        10.3.9.90
nvidia-cusolver-cu12      11.7.3.90
nvidia-cusparse-cu12      12.5.8.93
nvidia-cusparselt-cu12    0.7.1
nvidia-cutlass-dsl        4.2.1
nvidia-ml-py              13.580.82
nvidia-nccl-cu12          2.27.3
nvidia-nvjitlink-cu12     12.8.93
nvidia-nvtx-cu12          12.8.90
openai                    1.99.1
openai-harmony            0.0.4
orjson                    3.11.4
outlines                  0.1.11
outlines_core             0.1.26
packaging                 25.0
pandas                    2.3.3
parso                     0.8.5
partial-json-parser       0.2.1.1.post6
pexpect                   4.9.0
pillow                    12.0.0
pip                       25.3
prometheus_client         0.23.1
prompt_toolkit            3.0.52
propcache                 0.4.1
protobuf                  6.33.0
psutil                    7.1.2
ptyprocess                0.7.0
pure_eval                 0.2.3
py-spy                    0.4.1
pyarrow                   22.0.0
pybase64                  1.4.2
pycountry                 24.6.1
pycparser                 2.23
pycryptodomex             3.23.0
pydantic                  2.12.3
pydantic_core             2.41.4
Pygments                  2.19.2
pyproject_hooks           1.2.0
python-dateutil           2.9.0.post0
python-multipart          0.0.20
pytz                      2025.2
PyYAML                    6.0.3
pyzmq                     27.1.0
referencing               0.37.0
regex                     2025.10.23
requests                  2.32.5
rpds-py                   0.28.0
safetensors               0.6.2
scipy                     1.16.3
sentencepiece             0.2.1
setproctitle              1.3.7
setuptools                80.9.0
sgl-kernel                0.3.16.post4
sglang                    0.5.4.post1   /home/m/Desktop/sglang/sglang/python
six                       1.17.0
sniffio                   1.3.1
soundfile                 0.13.1
stack-data                0.6.3
starlette                 0.49.1
sympy                     1.14.0
tabulate                  0.9.0
tiktoken                  0.12.0
timm                      1.0.16
tokenizers                0.22.1
torch                     2.8.0
torch_memory_saver        0.0.9
torchao                   0.9.0
torchaudio                2.8.0
torchvision               0.23.0
tqdm                      4.67.1
traitlets                 5.14.3
transformers              4.57.1
triton                    3.4.0
typing_extensions         4.15.0
typing-inspection         0.4.2
tzdata                    2025.2
urllib3                   2.5.0
uvicorn                   0.38.0
uvloop                    0.22.1
wcwidth                   0.2.14
xgrammar                  0.1.25
xxhash                    3.6.0
yarl                      1.22.0
(venv) m@m-HP-Z440-Workstation:~/Desktop/sglang/sglang$ 






1955  cd sglang
 1956  python
 1957  git clone -b v0.5.4.post1 https://github.com/sgl-project/sglang.git
 1958  cd sglang
 1959  pip install --upgrade pip
 1960  pip install -e "python"
 1961  /home/m/huggingface/Qwen3-0.6B
 1962  python3 -m sglang.launch_server --model-path /home/m/huggingface/Qwen3-0.6B --mem-fraction-static 0.5
 1963  python3 -m sglang.launch_server --model-path /home/m/huggingface/Qwen3-0.6B --mem-fraction-static 0.8
 1964  python
 1965  tokenizer = AutoTokenizer.from_pretrained("facebook/opt-125m")
 1966  input = tokenizer.apply_chat_template(
 1967  )
 1968  sampling_params = {
 1969  }
 1970  result = llm.generate(prompt=input, sampling_params=sampling_params)
 1971  generated_text = result["text"]  # Assume there is only one prompt
 1972  print_highlight("==== Original Output ====")
 1973  print_highlight(generated_text)
 1974  parser = ReasoningParser("deepseek-r1")
 1975  reasoning_text, text = parser.parse_non_stream(generated_text)
 1976  print_highlight("==== Reasoning ====")
 1977  print_highlight(reasoning_text)
 1978  print_highlight("==== Text ====")
 1979  print_highlight(text)
 1980  GL_ENABLE_JIT_DEEPGEMM=0 python -m sglang.launch_server --model /home/m/huggingface/Qwen3-0.6B --tp 1  --host 0.0.0.0 --port  5000 --mem-fraction-static 0.8 --context-length 64  --enable-metrics  --attention-backend triton --chunked-prefill-size 500
 1981  nvidia-smi
 1982  dir
 1983  cd sglang
 1984  python
 1985  cd sglang
 1986  GL_ENABLE_JIT_DEEPGEMM=0 python -m sglang.launch_server --model /home/m/huggingface/Qwen3-0.6B --tp 1  --host 0.0.0.0 --port  5000 --mem-fraction-static 0.8 --context-length 64  --enable-metrics  --attention-backend triton --chunked-prefill-size 500
 1987  source venv/bin/activate
 1988  cd sglang
 1989  python
 1990  source venv/bin/activate
 1991  dir
 1992  cd sglang
 1993  curl http://localhost:5000/generate   -H "Content-Type: application/json"   -d '{
    "text": "The capital of France is",
    "sampling_params": {
      "temperature": 0.7,
      "max_new_tokens": 16
    }
  }'
 1994  nvidia-smi
 1995  pip list
 1996  history
(venv) m@m-HP-Z440-Workstation:~/Desktop/sglang/sglang$ 




